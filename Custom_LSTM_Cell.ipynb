{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUZH5LOfmt20"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "b2UbUpEQmrjG",
        "outputId": "1cbfab8f-8df7-4e54-cca7-543fff34d23b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 500,\n        \"samples\": [\n          \"1982-05-20\",\n          \"1981-03-30\",\n          \"1982-06-09\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10767765344237705,\n        \"min\": 0.1986607164144516,\n        \"max\": 0.6428571343421936,\n        \"num_unique_values\": 165,\n        \"samples\": [\n          0.2790178656578064,\n          0.3794642984867096,\n          0.2767857015132904\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10918741784220459,\n        \"min\": 0.1986607164144516,\n        \"max\": 0.6450892686843872,\n        \"num_unique_values\": 165,\n        \"samples\": [\n          0.2522321343421936,\n          0.4107142984867096,\n          0.2767857015132904\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10685639943686188,\n        \"min\": 0.1964285671710968,\n        \"max\": 0.6428571343421936,\n        \"num_unique_values\": 162,\n        \"samples\": [\n          0.4196428656578064,\n          0.34375,\n          0.28125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10809514466027073,\n        \"min\": 0.1964285671710968,\n        \"max\": 0.6428571343421936,\n        \"num_unique_values\": 164,\n        \"samples\": [\n          0.2633928656578064,\n          0.3348214328289032,\n          0.28125\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Adj Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08564806647901811,\n        \"min\": 0.1556382328271865,\n        \"max\": 0.509361207485199,\n        \"num_unique_values\": 164,\n        \"samples\": [\n          0.208696573972702,\n          0.2652923166751861,\n          0.2228455245494842\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14220752,\n        \"min\": 1041600,\n        \"max\": 117258400,\n        \"num_unique_values\": 472,\n        \"samples\": [\n          1344000,\n          7851200,\n          5941600\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"%Change\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.1892758844697204,\n        \"min\": -12.121216105801668,\n        \"max\": 10.389605435458105,\n        \"num_unique_values\": 384,\n        \"samples\": [\n          -1.4084546770799067,\n          -1.3422856679461082,\n          1.0752719045995063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ChgCat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Binary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-990025c8-9c72-47df-89bd-44b4436e3071\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>%Change</th>\n",
              "      <th>ChgCat</th>\n",
              "      <th>Binary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1980-12-12</td>\n",
              "      <td>0.513393</td>\n",
              "      <td>0.515625</td>\n",
              "      <td>0.513393</td>\n",
              "      <td>0.513393</td>\n",
              "      <td>0.406782</td>\n",
              "      <td>117258400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1980-12-15</td>\n",
              "      <td>0.488839</td>\n",
              "      <td>0.488839</td>\n",
              "      <td>0.486607</td>\n",
              "      <td>0.486607</td>\n",
              "      <td>0.385558</td>\n",
              "      <td>43971200</td>\n",
              "      <td>-4.782608</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1980-12-16</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>0.450893</td>\n",
              "      <td>0.450893</td>\n",
              "      <td>0.357260</td>\n",
              "      <td>26432000</td>\n",
              "      <td>-7.305938</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1980-12-17</td>\n",
              "      <td>0.462054</td>\n",
              "      <td>0.464286</td>\n",
              "      <td>0.462054</td>\n",
              "      <td>0.462054</td>\n",
              "      <td>0.366103</td>\n",
              "      <td>21610400</td>\n",
              "      <td>1.970442</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1980-12-18</td>\n",
              "      <td>0.475446</td>\n",
              "      <td>0.477679</td>\n",
              "      <td>0.475446</td>\n",
              "      <td>0.475446</td>\n",
              "      <td>0.376715</td>\n",
              "      <td>18362400</td>\n",
              "      <td>2.898553</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-990025c8-9c72-47df-89bd-44b4436e3071')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-990025c8-9c72-47df-89bd-44b4436e3071 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-990025c8-9c72-47df-89bd-44b4436e3071');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3d5904ac-a438-480e-8f3d-b7732dfc0274\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3d5904ac-a438-480e-8f3d-b7732dfc0274')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3d5904ac-a438-480e-8f3d-b7732dfc0274 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         Date      Open      High       Low     Close  Adj Close     Volume  \\\n",
              "0  1980-12-12  0.513393  0.515625  0.513393  0.513393   0.406782  117258400   \n",
              "1  1980-12-15  0.488839  0.488839  0.486607  0.486607   0.385558   43971200   \n",
              "2  1980-12-16  0.453125  0.453125  0.450893  0.450893   0.357260   26432000   \n",
              "3  1980-12-17  0.462054  0.464286  0.462054  0.462054   0.366103   21610400   \n",
              "4  1980-12-18  0.475446  0.477679  0.475446  0.475446   0.376715   18362400   \n",
              "\n",
              "    %Change  ChgCat  Binary  \n",
              "0  0.000000       1       0  \n",
              "1 -4.782608       1       0  \n",
              "2 -7.305938       0       0  \n",
              "3  1.970442       2       1  \n",
              "4  2.898553       2       1  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['%Change'] = (data['Open'].pct_change() * 100).fillna(0)\n",
        "\n",
        "# Categorise percentage change into ranges\n",
        "def categorise_change(percentage_change):\n",
        "    if percentage_change > 10:\n",
        "        return 4\n",
        "    elif percentage_change > 5:\n",
        "        return 3\n",
        "    elif percentage_change > 0:\n",
        "        return 2\n",
        "    elif percentage_change > -5:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "def binary_cat(val):\n",
        "    if val > 0:\n",
        "      return 1\n",
        "    return 0\n",
        "data['ChgCat'] = data['%Change'].apply(lambda x: categorise_change(x))\n",
        "data['Binary'] = data['%Change'].apply(lambda x: binary_cat(x))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "sWwwg_R_neC6",
        "outputId": "5eb2705a-5042-4530-c543-9ac49b61a2af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"X_train\",\n  \"rows\": 400,\n  \"fields\": [\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24125472443688395,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 151,\n        \"samples\": [\n          0.8442211593029166,\n          0.12060299400355934,\n          0.733668412004126\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24146972781836848,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 150,\n        \"samples\": [\n          0.7999999599456784,\n          0.65500004439354,\n          0.8750000751018532\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24356743605476974,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 152,\n        \"samples\": [\n          0.09999998998641924,\n          0.2550000061750415,\n          0.539999999332428\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23866784827968124,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 150,\n        \"samples\": [\n          0.5600000053405763,\n          0.65500004439354,\n          0.8750000751018532\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12093976898995244,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 377,\n        \"samples\": [\n          0.03869320098299041,\n          0.08610803257360382,\n          0.6989350937213897\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "X_train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-44d4b856-85e6-49ce-ab60-5902d924abda\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>0.321608</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.046788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>0.135678</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.130</td>\n",
              "      <td>0.065485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>0.085427</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.141377</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>0.743719</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.750</td>\n",
              "      <td>0.745</td>\n",
              "      <td>0.023370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.698493</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.695</td>\n",
              "      <td>0.690</td>\n",
              "      <td>0.090397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>0.311558</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.315</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.154725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>0.266332</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.270</td>\n",
              "      <td>0.047656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0.587940</td>\n",
              "      <td>0.585</td>\n",
              "      <td>0.585</td>\n",
              "      <td>0.585</td>\n",
              "      <td>0.017443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.587940</td>\n",
              "      <td>0.590</td>\n",
              "      <td>0.590</td>\n",
              "      <td>0.590</td>\n",
              "      <td>0.067075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>0.291457</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.295</td>\n",
              "      <td>0.215921</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44d4b856-85e6-49ce-ab60-5902d924abda')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-44d4b856-85e6-49ce-ab60-5902d924abda button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-44d4b856-85e6-49ce-ab60-5902d924abda');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5b9b0f10-cefd-4bbb-8c19-742f5a997375\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b9b0f10-cefd-4bbb-8c19-742f5a997375')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5b9b0f10-cefd-4bbb-8c19-742f5a997375 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_c2a63762-f88f-42a0-aa49-55dac046d02f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('X_train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c2a63762-f88f-42a0-aa49-55dac046d02f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('X_train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         Open  Close   High    Low    Volume\n",
              "177  0.321608  0.325  0.325  0.325  0.046788\n",
              "197  0.135678  0.130  0.135  0.130  0.065485\n",
              "418  0.085427  0.090  0.090  0.090  0.141377\n",
              "132  0.743719  0.745  0.750  0.745  0.023370\n",
              "33   0.698493  0.690  0.695  0.690  0.090397\n",
              "..        ...    ...    ...    ...       ...\n",
              "251  0.311558  0.310  0.315  0.310  0.154725\n",
              "192  0.266332  0.270  0.270  0.270  0.047656\n",
              "71   0.587940  0.585  0.585  0.585  0.017443\n",
              "87   0.587940  0.590  0.590  0.590  0.067075\n",
              "435  0.291457  0.295  0.295  0.295  0.215921\n",
              "\n",
              "[400 rows x 5 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "data = data.drop(columns = ['Date'],axis = 1)\n",
        "scaler = MinMaxScaler()\n",
        "data_normalized = scaler.fit_transform(data)\n",
        "\n",
        "numerical_cols = ['Open','Close','High','Low','Volume']\n",
        "scaler = MinMaxScaler()\n",
        "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "data['Cat_encoded'] = encoder.fit_transform(data['ChgCat'])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    data[['Open','Close','High','Low','Volume']], data[\"Cat_encoded\"], test_size=0.2, random_state=1000\n",
        ")\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uesmvlUoiZ1",
        "outputId": "b644fcdd-65a1-497b-cc1d-6e5de7193f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: 0%\n",
            "Internal Progress: Current epoch is 0, 0.0%\n",
            "Internal Progress: Current epoch is 0, 0.25%\n",
            "Internal Progress: Current epoch is 0, 0.5%\n",
            "Internal Progress: Current epoch is 0, 0.75%\n",
            "Internal Progress: Current epoch is 0, 1.0%\n",
            "Internal Progress: Current epoch is 0, 1.25%\n",
            "Internal Progress: Current epoch is 0, 1.5%\n",
            "Internal Progress: Current epoch is 0, 1.7500000000000002%\n",
            "Internal Progress: Current epoch is 0, 2.0%\n",
            "Internal Progress: Current epoch is 0, 2.25%\n",
            "Internal Progress: Current epoch is 0, 2.5%\n",
            "Internal Progress: Current epoch is 0, 2.75%\n",
            "Internal Progress: Current epoch is 0, 3.0%\n",
            "Internal Progress: Current epoch is 0, 3.25%\n",
            "Internal Progress: Current epoch is 0, 3.5000000000000004%\n",
            "Internal Progress: Current epoch is 0, 3.75%\n",
            "Internal Progress: Current epoch is 0, 4.0%\n",
            "Internal Progress: Current epoch is 0, 4.25%\n",
            "Internal Progress: Current epoch is 0, 4.5%\n",
            "Internal Progress: Current epoch is 0, 4.75%\n",
            "Internal Progress: Current epoch is 0, 5.0%\n",
            "Internal Progress: Current epoch is 0, 5.25%\n",
            "Internal Progress: Current epoch is 0, 5.5%\n",
            "Internal Progress: Current epoch is 0, 5.75%\n",
            "Internal Progress: Current epoch is 0, 6.0%\n",
            "Internal Progress: Current epoch is 0, 6.25%\n",
            "Internal Progress: Current epoch is 0, 6.5%\n",
            "Internal Progress: Current epoch is 0, 6.75%\n",
            "Internal Progress: Current epoch is 0, 7.000000000000001%\n",
            "Internal Progress: Current epoch is 0, 7.249999999999999%\n",
            "Internal Progress: Current epoch is 0, 7.5%\n",
            "Internal Progress: Current epoch is 0, 7.75%\n",
            "Internal Progress: Current epoch is 0, 8.0%\n",
            "Internal Progress: Current epoch is 0, 8.25%\n",
            "Internal Progress: Current epoch is 0, 8.5%\n",
            "Internal Progress: Current epoch is 0, 8.75%\n",
            "Internal Progress: Current epoch is 0, 9.0%\n",
            "Internal Progress: Current epoch is 0, 9.25%\n",
            "Internal Progress: Current epoch is 0, 9.5%\n",
            "Internal Progress: Current epoch is 0, 9.75%\n",
            "Internal Progress: Current epoch is 0, 10.0%\n",
            "Internal Progress: Current epoch is 0, 10.25%\n",
            "Internal Progress: Current epoch is 0, 10.5%\n",
            "Internal Progress: Current epoch is 0, 10.75%\n",
            "Internal Progress: Current epoch is 0, 11.0%\n",
            "Internal Progress: Current epoch is 0, 11.25%\n",
            "Internal Progress: Current epoch is 0, 11.5%\n",
            "Internal Progress: Current epoch is 0, 11.75%\n",
            "Internal Progress: Current epoch is 0, 12.0%\n",
            "Internal Progress: Current epoch is 0, 12.25%\n",
            "Internal Progress: Current epoch is 0, 12.5%\n",
            "Internal Progress: Current epoch is 0, 12.75%\n",
            "Internal Progress: Current epoch is 0, 13.0%\n",
            "Internal Progress: Current epoch is 0, 13.25%\n",
            "Internal Progress: Current epoch is 0, 13.5%\n",
            "Internal Progress: Current epoch is 0, 13.750000000000002%\n",
            "Internal Progress: Current epoch is 0, 14.000000000000002%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-147-6cdfee3ebf79>:27: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Internal Progress: Current epoch is 0, 14.249999999999998%\n",
            "Internal Progress: Current epoch is 0, 14.499999999999998%\n",
            "Internal Progress: Current epoch is 0, 14.75%\n",
            "Internal Progress: Current epoch is 0, 15.0%\n",
            "Internal Progress: Current epoch is 0, 15.25%\n",
            "Internal Progress: Current epoch is 0, 15.5%\n",
            "Internal Progress: Current epoch is 0, 15.75%\n",
            "Internal Progress: Current epoch is 0, 16.0%\n",
            "Internal Progress: Current epoch is 0, 16.25%\n",
            "Internal Progress: Current epoch is 0, 16.5%\n",
            "Internal Progress: Current epoch is 0, 16.75%\n",
            "Internal Progress: Current epoch is 0, 17.0%\n",
            "Internal Progress: Current epoch is 0, 17.25%\n",
            "Internal Progress: Current epoch is 0, 17.5%\n",
            "Internal Progress: Current epoch is 0, 17.75%\n",
            "Internal Progress: Current epoch is 0, 18.0%\n",
            "Internal Progress: Current epoch is 0, 18.25%\n",
            "Internal Progress: Current epoch is 0, 18.5%\n",
            "Internal Progress: Current epoch is 0, 18.75%\n",
            "Internal Progress: Current epoch is 0, 19.0%\n",
            "Internal Progress: Current epoch is 0, 19.25%\n",
            "Internal Progress: Current epoch is 0, 19.5%\n",
            "Internal Progress: Current epoch is 0, 19.75%\n",
            "Internal Progress: Current epoch is 0, 20.0%\n",
            "Internal Progress: Current epoch is 0, 20.25%\n",
            "Internal Progress: Current epoch is 0, 20.5%\n",
            "Internal Progress: Current epoch is 0, 20.75%\n",
            "Internal Progress: Current epoch is 0, 21.0%\n",
            "Internal Progress: Current epoch is 0, 21.25%\n",
            "Internal Progress: Current epoch is 0, 21.5%\n",
            "Internal Progress: Current epoch is 0, 21.75%\n",
            "Internal Progress: Current epoch is 0, 22.0%\n",
            "Internal Progress: Current epoch is 0, 22.25%\n",
            "Internal Progress: Current epoch is 0, 22.5%\n",
            "Internal Progress: Current epoch is 0, 22.75%\n",
            "Internal Progress: Current epoch is 0, 23.0%\n",
            "Internal Progress: Current epoch is 0, 23.25%\n",
            "Internal Progress: Current epoch is 0, 23.5%\n",
            "Internal Progress: Current epoch is 0, 23.75%\n",
            "Internal Progress: Current epoch is 0, 24.0%\n",
            "Internal Progress: Current epoch is 0, 24.25%\n",
            "Internal Progress: Current epoch is 0, 24.5%\n",
            "Internal Progress: Current epoch is 0, 24.75%\n",
            "Internal Progress: Current epoch is 0, 25.0%\n",
            "Internal Progress: Current epoch is 0, 25.25%\n",
            "Internal Progress: Current epoch is 0, 25.5%\n",
            "Internal Progress: Current epoch is 0, 25.75%\n",
            "Internal Progress: Current epoch is 0, 26.0%\n",
            "Internal Progress: Current epoch is 0, 26.25%\n",
            "Internal Progress: Current epoch is 0, 26.5%\n",
            "Internal Progress: Current epoch is 0, 26.75%\n",
            "Internal Progress: Current epoch is 0, 27.0%\n",
            "Internal Progress: Current epoch is 0, 27.250000000000004%\n",
            "Internal Progress: Current epoch is 0, 27.500000000000004%\n",
            "Internal Progress: Current epoch is 0, 27.750000000000004%\n",
            "Internal Progress: Current epoch is 0, 28.000000000000004%\n",
            "Internal Progress: Current epoch is 0, 28.249999999999996%\n",
            "Internal Progress: Current epoch is 0, 28.499999999999996%\n",
            "Internal Progress: Current epoch is 0, 28.749999999999996%\n",
            "Internal Progress: Current epoch is 0, 28.999999999999996%\n",
            "Internal Progress: Current epoch is 0, 29.25%\n",
            "Internal Progress: Current epoch is 0, 29.5%\n",
            "Internal Progress: Current epoch is 0, 29.75%\n",
            "Internal Progress: Current epoch is 0, 30.0%\n",
            "Internal Progress: Current epoch is 0, 30.25%\n",
            "Internal Progress: Current epoch is 0, 30.5%\n",
            "Internal Progress: Current epoch is 0, 30.75%\n",
            "Internal Progress: Current epoch is 0, 31.0%\n",
            "Internal Progress: Current epoch is 0, 31.25%\n",
            "Internal Progress: Current epoch is 0, 31.5%\n",
            "Internal Progress: Current epoch is 0, 31.75%\n",
            "Internal Progress: Current epoch is 0, 32.0%\n",
            "Internal Progress: Current epoch is 0, 32.25%\n",
            "Internal Progress: Current epoch is 0, 32.5%\n",
            "Internal Progress: Current epoch is 0, 32.75%\n",
            "Internal Progress: Current epoch is 0, 33.0%\n",
            "Internal Progress: Current epoch is 0, 33.25%\n",
            "Internal Progress: Current epoch is 0, 33.5%\n",
            "Internal Progress: Current epoch is 0, 33.75%\n",
            "Internal Progress: Current epoch is 0, 34.0%\n",
            "Internal Progress: Current epoch is 0, 34.25%\n",
            "Internal Progress: Current epoch is 0, 34.5%\n",
            "Internal Progress: Current epoch is 0, 34.75%\n",
            "Internal Progress: Current epoch is 0, 35.0%\n",
            "Internal Progress: Current epoch is 0, 35.25%\n",
            "Internal Progress: Current epoch is 0, 35.5%\n",
            "Internal Progress: Current epoch is 0, 35.75%\n",
            "Internal Progress: Current epoch is 0, 36.0%\n",
            "Internal Progress: Current epoch is 0, 36.25%\n",
            "Internal Progress: Current epoch is 0, 36.5%\n",
            "Internal Progress: Current epoch is 0, 36.75%\n",
            "Internal Progress: Current epoch is 0, 37.0%\n",
            "Internal Progress: Current epoch is 0, 37.25%\n",
            "Internal Progress: Current epoch is 0, 37.5%\n",
            "Internal Progress: Current epoch is 0, 37.75%\n",
            "Internal Progress: Current epoch is 0, 38.0%\n",
            "Internal Progress: Current epoch is 0, 38.25%\n",
            "Internal Progress: Current epoch is 0, 38.5%\n",
            "Internal Progress: Current epoch is 0, 38.75%\n",
            "Internal Progress: Current epoch is 0, 39.0%\n",
            "Internal Progress: Current epoch is 0, 39.25%\n",
            "Internal Progress: Current epoch is 0, 39.5%\n",
            "Internal Progress: Current epoch is 0, 39.75%\n",
            "Internal Progress: Current epoch is 0, 40.0%\n",
            "Internal Progress: Current epoch is 0, 40.25%\n",
            "Internal Progress: Current epoch is 0, 40.5%\n",
            "Internal Progress: Current epoch is 0, 40.75%\n",
            "Internal Progress: Current epoch is 0, 41.0%\n",
            "Internal Progress: Current epoch is 0, 41.25%\n",
            "Internal Progress: Current epoch is 0, 41.5%\n",
            "Internal Progress: Current epoch is 0, 41.75%\n",
            "Internal Progress: Current epoch is 0, 42.0%\n",
            "Internal Progress: Current epoch is 0, 42.25%\n",
            "Internal Progress: Current epoch is 0, 42.5%\n",
            "Internal Progress: Current epoch is 0, 42.75%\n",
            "Internal Progress: Current epoch is 0, 43.0%\n",
            "Internal Progress: Current epoch is 0, 43.25%\n",
            "Internal Progress: Current epoch is 0, 43.5%\n",
            "Internal Progress: Current epoch is 0, 43.75%\n",
            "Internal Progress: Current epoch is 0, 44.0%\n",
            "Internal Progress: Current epoch is 0, 44.25%\n",
            "Internal Progress: Current epoch is 0, 44.5%\n",
            "Internal Progress: Current epoch is 0, 44.75%\n",
            "Internal Progress: Current epoch is 0, 45.0%\n",
            "Internal Progress: Current epoch is 0, 45.25%\n",
            "Internal Progress: Current epoch is 0, 45.5%\n",
            "Internal Progress: Current epoch is 0, 45.75%\n",
            "Internal Progress: Current epoch is 0, 46.0%\n",
            "Internal Progress: Current epoch is 0, 46.25%\n",
            "Internal Progress: Current epoch is 0, 46.5%\n",
            "Internal Progress: Current epoch is 0, 46.75%\n",
            "Internal Progress: Current epoch is 0, 47.0%\n",
            "Internal Progress: Current epoch is 0, 47.25%\n",
            "Internal Progress: Current epoch is 0, 47.5%\n",
            "Internal Progress: Current epoch is 0, 47.75%\n",
            "Internal Progress: Current epoch is 0, 48.0%\n",
            "Internal Progress: Current epoch is 0, 48.25%\n",
            "Internal Progress: Current epoch is 0, 48.5%\n",
            "Internal Progress: Current epoch is 0, 48.75%\n",
            "Internal Progress: Current epoch is 0, 49.0%\n",
            "Internal Progress: Current epoch is 0, 49.25%\n",
            "Internal Progress: Current epoch is 0, 49.5%\n",
            "Internal Progress: Current epoch is 0, 49.75%\n",
            "Internal Progress: Current epoch is 0, 50.0%\n",
            "Internal Progress: Current epoch is 0, 50.24999999999999%\n",
            "Internal Progress: Current epoch is 0, 50.5%\n",
            "Internal Progress: Current epoch is 0, 50.74999999999999%\n",
            "Internal Progress: Current epoch is 0, 51.0%\n",
            "Internal Progress: Current epoch is 0, 51.24999999999999%\n",
            "Internal Progress: Current epoch is 0, 51.5%\n",
            "Internal Progress: Current epoch is 0, 51.74999999999999%\n",
            "Internal Progress: Current epoch is 0, 52.0%\n",
            "Internal Progress: Current epoch is 0, 52.25%\n",
            "Internal Progress: Current epoch is 0, 52.5%\n",
            "Internal Progress: Current epoch is 0, 52.75%\n",
            "Internal Progress: Current epoch is 0, 53.0%\n",
            "Internal Progress: Current epoch is 0, 53.25%\n",
            "Internal Progress: Current epoch is 0, 53.5%\n",
            "Internal Progress: Current epoch is 0, 53.75%\n",
            "Internal Progress: Current epoch is 0, 54.0%\n",
            "Internal Progress: Current epoch is 0, 54.25%\n",
            "Internal Progress: Current epoch is 0, 54.50000000000001%\n",
            "Internal Progress: Current epoch is 0, 54.75%\n",
            "Internal Progress: Current epoch is 0, 55.00000000000001%\n",
            "Internal Progress: Current epoch is 0, 55.25%\n",
            "Internal Progress: Current epoch is 0, 55.50000000000001%\n",
            "Internal Progress: Current epoch is 0, 55.75%\n",
            "Internal Progress: Current epoch is 0, 56.00000000000001%\n",
            "Internal Progress: Current epoch is 0, 56.25%\n",
            "Internal Progress: Current epoch is 0, 56.49999999999999%\n",
            "Internal Progress: Current epoch is 0, 56.75%\n",
            "Internal Progress: Current epoch is 0, 56.99999999999999%\n",
            "Internal Progress: Current epoch is 0, 57.25%\n",
            "Internal Progress: Current epoch is 0, 57.49999999999999%\n",
            "Internal Progress: Current epoch is 0, 57.75%\n",
            "Internal Progress: Current epoch is 0, 57.99999999999999%\n",
            "Internal Progress: Current epoch is 0, 58.25%\n",
            "Internal Progress: Current epoch is 0, 58.5%\n",
            "Internal Progress: Current epoch is 0, 58.75%\n",
            "Internal Progress: Current epoch is 0, 59.0%\n",
            "Internal Progress: Current epoch is 0, 59.25%\n",
            "Internal Progress: Current epoch is 0, 59.5%\n",
            "Internal Progress: Current epoch is 0, 59.75%\n",
            "Internal Progress: Current epoch is 0, 60.0%\n",
            "Internal Progress: Current epoch is 0, 60.25%\n",
            "Internal Progress: Current epoch is 0, 60.5%\n",
            "Internal Progress: Current epoch is 0, 60.75000000000001%\n",
            "Internal Progress: Current epoch is 0, 61.0%\n",
            "Internal Progress: Current epoch is 0, 61.25000000000001%\n",
            "Internal Progress: Current epoch is 0, 61.5%\n",
            "Internal Progress: Current epoch is 0, 61.75000000000001%\n",
            "Internal Progress: Current epoch is 0, 62.0%\n",
            "Internal Progress: Current epoch is 0, 62.25000000000001%\n",
            "Internal Progress: Current epoch is 0, 62.5%\n",
            "Internal Progress: Current epoch is 0, 62.74999999999999%\n",
            "Internal Progress: Current epoch is 0, 63.0%\n",
            "Internal Progress: Current epoch is 0, 63.24999999999999%\n",
            "Internal Progress: Current epoch is 0, 63.5%\n",
            "Internal Progress: Current epoch is 0, 63.74999999999999%\n",
            "Internal Progress: Current epoch is 0, 64.0%\n",
            "Internal Progress: Current epoch is 0, 64.25%\n",
            "Internal Progress: Current epoch is 0, 64.5%\n",
            "Internal Progress: Current epoch is 0, 64.75%\n",
            "Internal Progress: Current epoch is 0, 65.0%\n",
            "Internal Progress: Current epoch is 0, 65.25%\n",
            "Internal Progress: Current epoch is 0, 65.5%\n",
            "Internal Progress: Current epoch is 0, 65.75%\n",
            "Internal Progress: Current epoch is 0, 66.0%\n",
            "Internal Progress: Current epoch is 0, 66.25%\n",
            "Internal Progress: Current epoch is 0, 66.5%\n",
            "Internal Progress: Current epoch is 0, 66.75%\n",
            "Internal Progress: Current epoch is 0, 67.0%\n",
            "Internal Progress: Current epoch is 0, 67.25%\n",
            "Internal Progress: Current epoch is 0, 67.5%\n",
            "Internal Progress: Current epoch is 0, 67.75%\n",
            "Internal Progress: Current epoch is 0, 68.0%\n",
            "Internal Progress: Current epoch is 0, 68.25%\n",
            "Internal Progress: Current epoch is 0, 68.5%\n",
            "Internal Progress: Current epoch is 0, 68.75%\n",
            "Internal Progress: Current epoch is 0, 69.0%\n",
            "Internal Progress: Current epoch is 0, 69.25%\n",
            "Internal Progress: Current epoch is 0, 69.5%\n",
            "Internal Progress: Current epoch is 0, 69.75%\n",
            "Internal Progress: Current epoch is 0, 70.0%\n",
            "Internal Progress: Current epoch is 0, 70.25%\n",
            "Internal Progress: Current epoch is 0, 70.5%\n",
            "Internal Progress: Current epoch is 0, 70.75%\n",
            "Internal Progress: Current epoch is 0, 71.0%\n",
            "Internal Progress: Current epoch is 0, 71.25%\n",
            "Internal Progress: Current epoch is 0, 71.5%\n",
            "Internal Progress: Current epoch is 0, 71.75%\n",
            "Internal Progress: Current epoch is 0, 72.0%\n",
            "Internal Progress: Current epoch is 0, 72.25%\n",
            "Internal Progress: Current epoch is 0, 72.5%\n",
            "Internal Progress: Current epoch is 0, 72.75%\n",
            "Internal Progress: Current epoch is 0, 73.0%\n",
            "Internal Progress: Current epoch is 0, 73.25%\n",
            "Internal Progress: Current epoch is 0, 73.5%\n",
            "Internal Progress: Current epoch is 0, 73.75%\n",
            "Internal Progress: Current epoch is 0, 74.0%\n",
            "Internal Progress: Current epoch is 0, 74.25%\n",
            "Internal Progress: Current epoch is 0, 74.5%\n",
            "Internal Progress: Current epoch is 0, 74.75%\n",
            "Internal Progress: Current epoch is 0, 75.0%\n",
            "Internal Progress: Current epoch is 0, 75.25%\n",
            "Internal Progress: Current epoch is 0, 75.5%\n",
            "Internal Progress: Current epoch is 0, 75.75%\n",
            "Internal Progress: Current epoch is 0, 76.0%\n",
            "Internal Progress: Current epoch is 0, 76.25%\n",
            "Internal Progress: Current epoch is 0, 76.5%\n",
            "Internal Progress: Current epoch is 0, 76.75%\n",
            "Internal Progress: Current epoch is 0, 77.0%\n",
            "Internal Progress: Current epoch is 0, 77.25%\n",
            "Internal Progress: Current epoch is 0, 77.5%\n",
            "Internal Progress: Current epoch is 0, 77.75%\n",
            "Internal Progress: Current epoch is 0, 78.0%\n",
            "Internal Progress: Current epoch is 0, 78.25%\n",
            "Internal Progress: Current epoch is 0, 78.5%\n",
            "Internal Progress: Current epoch is 0, 78.75%\n",
            "Internal Progress: Current epoch is 0, 79.0%\n",
            "Internal Progress: Current epoch is 0, 79.25%\n",
            "Internal Progress: Current epoch is 0, 79.5%\n",
            "Internal Progress: Current epoch is 0, 79.75%\n",
            "Internal Progress: Current epoch is 0, 80.0%\n",
            "Internal Progress: Current epoch is 0, 80.25%\n",
            "Internal Progress: Current epoch is 0, 80.5%\n",
            "Internal Progress: Current epoch is 0, 80.75%\n",
            "Internal Progress: Current epoch is 0, 81.0%\n",
            "Internal Progress: Current epoch is 0, 81.25%\n",
            "Internal Progress: Current epoch is 0, 81.5%\n",
            "Internal Progress: Current epoch is 0, 81.75%\n",
            "Internal Progress: Current epoch is 0, 82.0%\n",
            "Internal Progress: Current epoch is 0, 82.25%\n",
            "Internal Progress: Current epoch is 0, 82.5%\n",
            "Internal Progress: Current epoch is 0, 82.75%\n",
            "Internal Progress: Current epoch is 0, 83.0%\n",
            "Internal Progress: Current epoch is 0, 83.25%\n",
            "Internal Progress: Current epoch is 0, 83.5%\n",
            "Internal Progress: Current epoch is 0, 83.75%\n",
            "Internal Progress: Current epoch is 0, 84.0%\n",
            "Internal Progress: Current epoch is 0, 84.25%\n",
            "Internal Progress: Current epoch is 0, 84.5%\n",
            "Internal Progress: Current epoch is 0, 84.75%\n",
            "Internal Progress: Current epoch is 0, 85.0%\n",
            "Internal Progress: Current epoch is 0, 85.25%\n",
            "Internal Progress: Current epoch is 0, 85.5%\n",
            "Internal Progress: Current epoch is 0, 85.75%\n",
            "Internal Progress: Current epoch is 0, 86.0%\n",
            "Internal Progress: Current epoch is 0, 86.25%\n",
            "Internal Progress: Current epoch is 0, 86.5%\n",
            "Internal Progress: Current epoch is 0, 86.75%\n",
            "Internal Progress: Current epoch is 0, 87.0%\n",
            "Internal Progress: Current epoch is 0, 87.25%\n",
            "Internal Progress: Current epoch is 0, 87.5%\n",
            "Internal Progress: Current epoch is 0, 87.75%\n",
            "Internal Progress: Current epoch is 0, 88.0%\n",
            "Internal Progress: Current epoch is 0, 88.25%\n",
            "Internal Progress: Current epoch is 0, 88.5%\n",
            "Internal Progress: Current epoch is 0, 88.75%\n",
            "Internal Progress: Current epoch is 0, 89.0%\n",
            "Internal Progress: Current epoch is 0, 89.25%\n",
            "Internal Progress: Current epoch is 0, 89.5%\n",
            "Internal Progress: Current epoch is 0, 89.75%\n",
            "Internal Progress: Current epoch is 0, 90.0%\n",
            "Internal Progress: Current epoch is 0, 90.25%\n",
            "Internal Progress: Current epoch is 0, 90.5%\n",
            "Internal Progress: Current epoch is 0, 90.75%\n",
            "Internal Progress: Current epoch is 0, 91.0%\n",
            "Internal Progress: Current epoch is 0, 91.25%\n",
            "Internal Progress: Current epoch is 0, 91.5%\n",
            "Internal Progress: Current epoch is 0, 91.75%\n",
            "Internal Progress: Current epoch is 0, 92.0%\n",
            "Internal Progress: Current epoch is 0, 92.25%\n",
            "Internal Progress: Current epoch is 0, 92.5%\n",
            "Internal Progress: Current epoch is 0, 92.75%\n",
            "Internal Progress: Current epoch is 0, 93.0%\n",
            "Internal Progress: Current epoch is 0, 93.25%\n",
            "Internal Progress: Current epoch is 0, 93.5%\n",
            "Internal Progress: Current epoch is 0, 93.75%\n",
            "Internal Progress: Current epoch is 0, 94.0%\n",
            "Internal Progress: Current epoch is 0, 94.25%\n",
            "Internal Progress: Current epoch is 0, 94.5%\n",
            "Internal Progress: Current epoch is 0, 94.75%\n",
            "Internal Progress: Current epoch is 0, 95.0%\n",
            "Internal Progress: Current epoch is 0, 95.25%\n",
            "Internal Progress: Current epoch is 0, 95.5%\n",
            "Internal Progress: Current epoch is 0, 95.75%\n",
            "Internal Progress: Current epoch is 0, 96.0%\n",
            "Internal Progress: Current epoch is 0, 96.25%\n",
            "Internal Progress: Current epoch is 0, 96.5%\n",
            "Internal Progress: Current epoch is 0, 96.75%\n",
            "Internal Progress: Current epoch is 0, 97.0%\n",
            "Internal Progress: Current epoch is 0, 97.25%\n",
            "Internal Progress: Current epoch is 0, 97.5%\n",
            "Internal Progress: Current epoch is 0, 97.75%\n",
            "Internal Progress: Current epoch is 0, 98.0%\n",
            "Internal Progress: Current epoch is 0, 98.25%\n",
            "Internal Progress: Current epoch is 0, 98.5%\n",
            "Internal Progress: Current epoch is 0, 98.75%\n",
            "Internal Progress: Current epoch is 0, 99.0%\n",
            "Internal Progress: Current epoch is 0, 99.25%\n",
            "Internal Progress: Current epoch is 0, 99.5%\n",
            "Internal Progress: Current epoch is 0, 99.75%\n",
            "Epoch 0, Loss: nan\n",
            "Progress: 1%\n",
            "Internal Progress: Current epoch is 1, 0.0%\n",
            "Internal Progress: Current epoch is 1, 0.25%\n",
            "Internal Progress: Current epoch is 1, 0.5%\n",
            "Internal Progress: Current epoch is 1, 0.75%\n",
            "Internal Progress: Current epoch is 1, 1.0%\n",
            "Internal Progress: Current epoch is 1, 1.25%\n",
            "Internal Progress: Current epoch is 1, 1.5%\n",
            "Internal Progress: Current epoch is 1, 1.7500000000000002%\n",
            "Internal Progress: Current epoch is 1, 2.0%\n",
            "Internal Progress: Current epoch is 1, 2.25%\n",
            "Internal Progress: Current epoch is 1, 2.5%\n",
            "Internal Progress: Current epoch is 1, 2.75%\n",
            "Internal Progress: Current epoch is 1, 3.0%\n",
            "Internal Progress: Current epoch is 1, 3.25%\n",
            "Internal Progress: Current epoch is 1, 3.5000000000000004%\n",
            "Internal Progress: Current epoch is 1, 3.75%\n",
            "Internal Progress: Current epoch is 1, 4.0%\n",
            "Internal Progress: Current epoch is 1, 4.25%\n",
            "Internal Progress: Current epoch is 1, 4.5%\n",
            "Internal Progress: Current epoch is 1, 4.75%\n",
            "Internal Progress: Current epoch is 1, 5.0%\n",
            "Internal Progress: Current epoch is 1, 5.25%\n",
            "Internal Progress: Current epoch is 1, 5.5%\n",
            "Internal Progress: Current epoch is 1, 5.75%\n",
            "Internal Progress: Current epoch is 1, 6.0%\n",
            "Internal Progress: Current epoch is 1, 6.25%\n",
            "Internal Progress: Current epoch is 1, 6.5%\n",
            "Internal Progress: Current epoch is 1, 6.75%\n",
            "Internal Progress: Current epoch is 1, 7.000000000000001%\n",
            "Internal Progress: Current epoch is 1, 7.249999999999999%\n",
            "Internal Progress: Current epoch is 1, 7.5%\n",
            "Internal Progress: Current epoch is 1, 7.75%\n",
            "Internal Progress: Current epoch is 1, 8.0%\n",
            "Internal Progress: Current epoch is 1, 8.25%\n",
            "Internal Progress: Current epoch is 1, 8.5%\n",
            "Internal Progress: Current epoch is 1, 8.75%\n",
            "Internal Progress: Current epoch is 1, 9.0%\n",
            "Internal Progress: Current epoch is 1, 9.25%\n",
            "Internal Progress: Current epoch is 1, 9.5%\n",
            "Internal Progress: Current epoch is 1, 9.75%\n",
            "Internal Progress: Current epoch is 1, 10.0%\n",
            "Internal Progress: Current epoch is 1, 10.25%\n",
            "Internal Progress: Current epoch is 1, 10.5%\n",
            "Internal Progress: Current epoch is 1, 10.75%\n",
            "Internal Progress: Current epoch is 1, 11.0%\n",
            "Internal Progress: Current epoch is 1, 11.25%\n",
            "Internal Progress: Current epoch is 1, 11.5%\n",
            "Internal Progress: Current epoch is 1, 11.75%\n",
            "Internal Progress: Current epoch is 1, 12.0%\n",
            "Internal Progress: Current epoch is 1, 12.25%\n",
            "Internal Progress: Current epoch is 1, 12.5%\n",
            "Internal Progress: Current epoch is 1, 12.75%\n",
            "Internal Progress: Current epoch is 1, 13.0%\n",
            "Internal Progress: Current epoch is 1, 13.25%\n",
            "Internal Progress: Current epoch is 1, 13.5%\n",
            "Internal Progress: Current epoch is 1, 13.750000000000002%\n",
            "Internal Progress: Current epoch is 1, 14.000000000000002%\n",
            "Internal Progress: Current epoch is 1, 14.249999999999998%\n",
            "Internal Progress: Current epoch is 1, 14.499999999999998%\n",
            "Internal Progress: Current epoch is 1, 14.75%\n",
            "Internal Progress: Current epoch is 1, 15.0%\n",
            "Internal Progress: Current epoch is 1, 15.25%\n",
            "Internal Progress: Current epoch is 1, 15.5%\n",
            "Internal Progress: Current epoch is 1, 15.75%\n",
            "Internal Progress: Current epoch is 1, 16.0%\n",
            "Internal Progress: Current epoch is 1, 16.25%\n",
            "Internal Progress: Current epoch is 1, 16.5%\n",
            "Internal Progress: Current epoch is 1, 16.75%\n",
            "Internal Progress: Current epoch is 1, 17.0%\n",
            "Internal Progress: Current epoch is 1, 17.25%\n",
            "Internal Progress: Current epoch is 1, 17.5%\n",
            "Internal Progress: Current epoch is 1, 17.75%\n",
            "Internal Progress: Current epoch is 1, 18.0%\n",
            "Internal Progress: Current epoch is 1, 18.25%\n",
            "Internal Progress: Current epoch is 1, 18.5%\n",
            "Internal Progress: Current epoch is 1, 18.75%\n",
            "Internal Progress: Current epoch is 1, 19.0%\n",
            "Internal Progress: Current epoch is 1, 19.25%\n",
            "Internal Progress: Current epoch is 1, 19.5%\n",
            "Internal Progress: Current epoch is 1, 19.75%\n",
            "Internal Progress: Current epoch is 1, 20.0%\n",
            "Internal Progress: Current epoch is 1, 20.25%\n",
            "Internal Progress: Current epoch is 1, 20.5%\n",
            "Internal Progress: Current epoch is 1, 20.75%\n",
            "Internal Progress: Current epoch is 1, 21.0%\n",
            "Internal Progress: Current epoch is 1, 21.25%\n",
            "Internal Progress: Current epoch is 1, 21.5%\n",
            "Internal Progress: Current epoch is 1, 21.75%\n",
            "Internal Progress: Current epoch is 1, 22.0%\n",
            "Internal Progress: Current epoch is 1, 22.25%\n",
            "Internal Progress: Current epoch is 1, 22.5%\n",
            "Internal Progress: Current epoch is 1, 22.75%\n",
            "Internal Progress: Current epoch is 1, 23.0%\n",
            "Internal Progress: Current epoch is 1, 23.25%\n",
            "Internal Progress: Current epoch is 1, 23.5%\n",
            "Internal Progress: Current epoch is 1, 23.75%\n",
            "Internal Progress: Current epoch is 1, 24.0%\n",
            "Internal Progress: Current epoch is 1, 24.25%\n",
            "Internal Progress: Current epoch is 1, 24.5%\n",
            "Internal Progress: Current epoch is 1, 24.75%\n",
            "Internal Progress: Current epoch is 1, 25.0%\n",
            "Internal Progress: Current epoch is 1, 25.25%\n",
            "Internal Progress: Current epoch is 1, 25.5%\n",
            "Internal Progress: Current epoch is 1, 25.75%\n",
            "Internal Progress: Current epoch is 1, 26.0%\n",
            "Internal Progress: Current epoch is 1, 26.25%\n",
            "Internal Progress: Current epoch is 1, 26.5%\n",
            "Internal Progress: Current epoch is 1, 26.75%\n",
            "Internal Progress: Current epoch is 1, 27.0%\n",
            "Internal Progress: Current epoch is 1, 27.250000000000004%\n",
            "Internal Progress: Current epoch is 1, 27.500000000000004%\n",
            "Internal Progress: Current epoch is 1, 27.750000000000004%\n",
            "Internal Progress: Current epoch is 1, 28.000000000000004%\n",
            "Internal Progress: Current epoch is 1, 28.249999999999996%\n",
            "Internal Progress: Current epoch is 1, 28.499999999999996%\n",
            "Internal Progress: Current epoch is 1, 28.749999999999996%\n",
            "Internal Progress: Current epoch is 1, 28.999999999999996%\n",
            "Internal Progress: Current epoch is 1, 29.25%\n",
            "Internal Progress: Current epoch is 1, 29.5%\n",
            "Internal Progress: Current epoch is 1, 29.75%\n",
            "Internal Progress: Current epoch is 1, 30.0%\n",
            "Internal Progress: Current epoch is 1, 30.25%\n",
            "Internal Progress: Current epoch is 1, 30.5%\n",
            "Internal Progress: Current epoch is 1, 30.75%\n",
            "Internal Progress: Current epoch is 1, 31.0%\n",
            "Internal Progress: Current epoch is 1, 31.25%\n",
            "Internal Progress: Current epoch is 1, 31.5%\n",
            "Internal Progress: Current epoch is 1, 31.75%\n",
            "Internal Progress: Current epoch is 1, 32.0%\n",
            "Internal Progress: Current epoch is 1, 32.25%\n",
            "Internal Progress: Current epoch is 1, 32.5%\n",
            "Internal Progress: Current epoch is 1, 32.75%\n",
            "Internal Progress: Current epoch is 1, 33.0%\n",
            "Internal Progress: Current epoch is 1, 33.25%\n",
            "Internal Progress: Current epoch is 1, 33.5%\n",
            "Internal Progress: Current epoch is 1, 33.75%\n",
            "Internal Progress: Current epoch is 1, 34.0%\n",
            "Internal Progress: Current epoch is 1, 34.25%\n",
            "Internal Progress: Current epoch is 1, 34.5%\n",
            "Internal Progress: Current epoch is 1, 34.75%\n",
            "Internal Progress: Current epoch is 1, 35.0%\n",
            "Internal Progress: Current epoch is 1, 35.25%\n",
            "Internal Progress: Current epoch is 1, 35.5%\n",
            "Internal Progress: Current epoch is 1, 35.75%\n",
            "Internal Progress: Current epoch is 1, 36.0%\n",
            "Internal Progress: Current epoch is 1, 36.25%\n",
            "Internal Progress: Current epoch is 1, 36.5%\n",
            "Internal Progress: Current epoch is 1, 36.75%\n",
            "Internal Progress: Current epoch is 1, 37.0%\n",
            "Internal Progress: Current epoch is 1, 37.25%\n",
            "Internal Progress: Current epoch is 1, 37.5%\n",
            "Internal Progress: Current epoch is 1, 37.75%\n",
            "Internal Progress: Current epoch is 1, 38.0%\n",
            "Internal Progress: Current epoch is 1, 38.25%\n",
            "Internal Progress: Current epoch is 1, 38.5%\n",
            "Internal Progress: Current epoch is 1, 38.75%\n",
            "Internal Progress: Current epoch is 1, 39.0%\n",
            "Internal Progress: Current epoch is 1, 39.25%\n",
            "Internal Progress: Current epoch is 1, 39.5%\n",
            "Internal Progress: Current epoch is 1, 39.75%\n",
            "Internal Progress: Current epoch is 1, 40.0%\n",
            "Internal Progress: Current epoch is 1, 40.25%\n",
            "Internal Progress: Current epoch is 1, 40.5%\n",
            "Internal Progress: Current epoch is 1, 40.75%\n",
            "Internal Progress: Current epoch is 1, 41.0%\n",
            "Internal Progress: Current epoch is 1, 41.25%\n",
            "Internal Progress: Current epoch is 1, 41.5%\n",
            "Internal Progress: Current epoch is 1, 41.75%\n",
            "Internal Progress: Current epoch is 1, 42.0%\n",
            "Internal Progress: Current epoch is 1, 42.25%\n",
            "Internal Progress: Current epoch is 1, 42.5%\n",
            "Internal Progress: Current epoch is 1, 42.75%\n",
            "Internal Progress: Current epoch is 1, 43.0%\n",
            "Internal Progress: Current epoch is 1, 43.25%\n",
            "Internal Progress: Current epoch is 1, 43.5%\n",
            "Internal Progress: Current epoch is 1, 43.75%\n",
            "Internal Progress: Current epoch is 1, 44.0%\n",
            "Internal Progress: Current epoch is 1, 44.25%\n",
            "Internal Progress: Current epoch is 1, 44.5%\n",
            "Internal Progress: Current epoch is 1, 44.75%\n",
            "Internal Progress: Current epoch is 1, 45.0%\n",
            "Internal Progress: Current epoch is 1, 45.25%\n",
            "Internal Progress: Current epoch is 1, 45.5%\n",
            "Internal Progress: Current epoch is 1, 45.75%\n",
            "Internal Progress: Current epoch is 1, 46.0%\n",
            "Internal Progress: Current epoch is 1, 46.25%\n",
            "Internal Progress: Current epoch is 1, 46.5%\n",
            "Internal Progress: Current epoch is 1, 46.75%\n",
            "Internal Progress: Current epoch is 1, 47.0%\n",
            "Internal Progress: Current epoch is 1, 47.25%\n",
            "Internal Progress: Current epoch is 1, 47.5%\n",
            "Internal Progress: Current epoch is 1, 47.75%\n",
            "Internal Progress: Current epoch is 1, 48.0%\n",
            "Internal Progress: Current epoch is 1, 48.25%\n",
            "Internal Progress: Current epoch is 1, 48.5%\n",
            "Internal Progress: Current epoch is 1, 48.75%\n",
            "Internal Progress: Current epoch is 1, 49.0%\n",
            "Internal Progress: Current epoch is 1, 49.25%\n",
            "Internal Progress: Current epoch is 1, 49.5%\n",
            "Internal Progress: Current epoch is 1, 49.75%\n",
            "Internal Progress: Current epoch is 1, 50.0%\n",
            "Internal Progress: Current epoch is 1, 50.24999999999999%\n",
            "Internal Progress: Current epoch is 1, 50.5%\n",
            "Internal Progress: Current epoch is 1, 50.74999999999999%\n",
            "Internal Progress: Current epoch is 1, 51.0%\n",
            "Internal Progress: Current epoch is 1, 51.24999999999999%\n",
            "Internal Progress: Current epoch is 1, 51.5%\n",
            "Internal Progress: Current epoch is 1, 51.74999999999999%\n",
            "Internal Progress: Current epoch is 1, 52.0%\n",
            "Internal Progress: Current epoch is 1, 52.25%\n",
            "Internal Progress: Current epoch is 1, 52.5%\n",
            "Internal Progress: Current epoch is 1, 52.75%\n",
            "Internal Progress: Current epoch is 1, 53.0%\n",
            "Internal Progress: Current epoch is 1, 53.25%\n",
            "Internal Progress: Current epoch is 1, 53.5%\n",
            "Internal Progress: Current epoch is 1, 53.75%\n",
            "Internal Progress: Current epoch is 1, 54.0%\n",
            "Internal Progress: Current epoch is 1, 54.25%\n",
            "Internal Progress: Current epoch is 1, 54.50000000000001%\n",
            "Internal Progress: Current epoch is 1, 54.75%\n",
            "Internal Progress: Current epoch is 1, 55.00000000000001%\n",
            "Internal Progress: Current epoch is 1, 55.25%\n",
            "Internal Progress: Current epoch is 1, 55.50000000000001%\n",
            "Internal Progress: Current epoch is 1, 55.75%\n",
            "Internal Progress: Current epoch is 1, 56.00000000000001%\n",
            "Internal Progress: Current epoch is 1, 56.25%\n",
            "Internal Progress: Current epoch is 1, 56.49999999999999%\n",
            "Internal Progress: Current epoch is 1, 56.75%\n",
            "Internal Progress: Current epoch is 1, 56.99999999999999%\n",
            "Internal Progress: Current epoch is 1, 57.25%\n",
            "Internal Progress: Current epoch is 1, 57.49999999999999%\n",
            "Internal Progress: Current epoch is 1, 57.75%\n",
            "Internal Progress: Current epoch is 1, 57.99999999999999%\n",
            "Internal Progress: Current epoch is 1, 58.25%\n",
            "Internal Progress: Current epoch is 1, 58.5%\n",
            "Internal Progress: Current epoch is 1, 58.75%\n",
            "Internal Progress: Current epoch is 1, 59.0%\n",
            "Internal Progress: Current epoch is 1, 59.25%\n",
            "Internal Progress: Current epoch is 1, 59.5%\n",
            "Internal Progress: Current epoch is 1, 59.75%\n",
            "Internal Progress: Current epoch is 1, 60.0%\n",
            "Internal Progress: Current epoch is 1, 60.25%\n",
            "Internal Progress: Current epoch is 1, 60.5%\n",
            "Internal Progress: Current epoch is 1, 60.75000000000001%\n",
            "Internal Progress: Current epoch is 1, 61.0%\n",
            "Internal Progress: Current epoch is 1, 61.25000000000001%\n",
            "Internal Progress: Current epoch is 1, 61.5%\n",
            "Internal Progress: Current epoch is 1, 61.75000000000001%\n",
            "Internal Progress: Current epoch is 1, 62.0%\n",
            "Internal Progress: Current epoch is 1, 62.25000000000001%\n",
            "Internal Progress: Current epoch is 1, 62.5%\n",
            "Internal Progress: Current epoch is 1, 62.74999999999999%\n",
            "Internal Progress: Current epoch is 1, 63.0%\n",
            "Internal Progress: Current epoch is 1, 63.24999999999999%\n",
            "Internal Progress: Current epoch is 1, 63.5%\n",
            "Internal Progress: Current epoch is 1, 63.74999999999999%\n",
            "Internal Progress: Current epoch is 1, 64.0%\n",
            "Internal Progress: Current epoch is 1, 64.25%\n",
            "Internal Progress: Current epoch is 1, 64.5%\n",
            "Internal Progress: Current epoch is 1, 64.75%\n",
            "Internal Progress: Current epoch is 1, 65.0%\n",
            "Internal Progress: Current epoch is 1, 65.25%\n",
            "Internal Progress: Current epoch is 1, 65.5%\n",
            "Internal Progress: Current epoch is 1, 65.75%\n",
            "Internal Progress: Current epoch is 1, 66.0%\n",
            "Internal Progress: Current epoch is 1, 66.25%\n",
            "Internal Progress: Current epoch is 1, 66.5%\n",
            "Internal Progress: Current epoch is 1, 66.75%\n",
            "Internal Progress: Current epoch is 1, 67.0%\n",
            "Internal Progress: Current epoch is 1, 67.25%\n",
            "Internal Progress: Current epoch is 1, 67.5%\n",
            "Internal Progress: Current epoch is 1, 67.75%\n",
            "Internal Progress: Current epoch is 1, 68.0%\n",
            "Internal Progress: Current epoch is 1, 68.25%\n",
            "Internal Progress: Current epoch is 1, 68.5%\n",
            "Internal Progress: Current epoch is 1, 68.75%\n",
            "Internal Progress: Current epoch is 1, 69.0%\n",
            "Internal Progress: Current epoch is 1, 69.25%\n",
            "Internal Progress: Current epoch is 1, 69.5%\n",
            "Internal Progress: Current epoch is 1, 69.75%\n",
            "Internal Progress: Current epoch is 1, 70.0%\n",
            "Internal Progress: Current epoch is 1, 70.25%\n",
            "Internal Progress: Current epoch is 1, 70.5%\n",
            "Internal Progress: Current epoch is 1, 70.75%\n",
            "Internal Progress: Current epoch is 1, 71.0%\n",
            "Internal Progress: Current epoch is 1, 71.25%\n",
            "Internal Progress: Current epoch is 1, 71.5%\n",
            "Internal Progress: Current epoch is 1, 71.75%\n",
            "Internal Progress: Current epoch is 1, 72.0%\n",
            "Internal Progress: Current epoch is 1, 72.25%\n",
            "Internal Progress: Current epoch is 1, 72.5%\n",
            "Internal Progress: Current epoch is 1, 72.75%\n",
            "Internal Progress: Current epoch is 1, 73.0%\n",
            "Internal Progress: Current epoch is 1, 73.25%\n",
            "Internal Progress: Current epoch is 1, 73.5%\n",
            "Internal Progress: Current epoch is 1, 73.75%\n",
            "Internal Progress: Current epoch is 1, 74.0%\n",
            "Internal Progress: Current epoch is 1, 74.25%\n",
            "Internal Progress: Current epoch is 1, 74.5%\n",
            "Internal Progress: Current epoch is 1, 74.75%\n",
            "Internal Progress: Current epoch is 1, 75.0%\n",
            "Internal Progress: Current epoch is 1, 75.25%\n",
            "Internal Progress: Current epoch is 1, 75.5%\n",
            "Internal Progress: Current epoch is 1, 75.75%\n",
            "Internal Progress: Current epoch is 1, 76.0%\n",
            "Internal Progress: Current epoch is 1, 76.25%\n",
            "Internal Progress: Current epoch is 1, 76.5%\n",
            "Internal Progress: Current epoch is 1, 76.75%\n",
            "Internal Progress: Current epoch is 1, 77.0%\n",
            "Internal Progress: Current epoch is 1, 77.25%\n",
            "Internal Progress: Current epoch is 1, 77.5%\n",
            "Internal Progress: Current epoch is 1, 77.75%\n",
            "Internal Progress: Current epoch is 1, 78.0%\n",
            "Internal Progress: Current epoch is 1, 78.25%\n",
            "Internal Progress: Current epoch is 1, 78.5%\n",
            "Internal Progress: Current epoch is 1, 78.75%\n",
            "Internal Progress: Current epoch is 1, 79.0%\n",
            "Internal Progress: Current epoch is 1, 79.25%\n",
            "Internal Progress: Current epoch is 1, 79.5%\n",
            "Internal Progress: Current epoch is 1, 79.75%\n",
            "Internal Progress: Current epoch is 1, 80.0%\n",
            "Internal Progress: Current epoch is 1, 80.25%\n",
            "Internal Progress: Current epoch is 1, 80.5%\n",
            "Internal Progress: Current epoch is 1, 80.75%\n",
            "Internal Progress: Current epoch is 1, 81.0%\n",
            "Internal Progress: Current epoch is 1, 81.25%\n",
            "Internal Progress: Current epoch is 1, 81.5%\n",
            "Internal Progress: Current epoch is 1, 81.75%\n",
            "Internal Progress: Current epoch is 1, 82.0%\n",
            "Internal Progress: Current epoch is 1, 82.25%\n",
            "Internal Progress: Current epoch is 1, 82.5%\n",
            "Internal Progress: Current epoch is 1, 82.75%\n",
            "Internal Progress: Current epoch is 1, 83.0%\n",
            "Internal Progress: Current epoch is 1, 83.25%\n",
            "Internal Progress: Current epoch is 1, 83.5%\n",
            "Internal Progress: Current epoch is 1, 83.75%\n",
            "Internal Progress: Current epoch is 1, 84.0%\n",
            "Internal Progress: Current epoch is 1, 84.25%\n",
            "Internal Progress: Current epoch is 1, 84.5%\n",
            "Internal Progress: Current epoch is 1, 84.75%\n",
            "Internal Progress: Current epoch is 1, 85.0%\n",
            "Internal Progress: Current epoch is 1, 85.25%\n",
            "Internal Progress: Current epoch is 1, 85.5%\n",
            "Internal Progress: Current epoch is 1, 85.75%\n",
            "Internal Progress: Current epoch is 1, 86.0%\n",
            "Internal Progress: Current epoch is 1, 86.25%\n",
            "Internal Progress: Current epoch is 1, 86.5%\n",
            "Internal Progress: Current epoch is 1, 86.75%\n",
            "Internal Progress: Current epoch is 1, 87.0%\n",
            "Internal Progress: Current epoch is 1, 87.25%\n",
            "Internal Progress: Current epoch is 1, 87.5%\n",
            "Internal Progress: Current epoch is 1, 87.75%\n",
            "Internal Progress: Current epoch is 1, 88.0%\n",
            "Internal Progress: Current epoch is 1, 88.25%\n",
            "Internal Progress: Current epoch is 1, 88.5%\n",
            "Internal Progress: Current epoch is 1, 88.75%\n",
            "Internal Progress: Current epoch is 1, 89.0%\n",
            "Internal Progress: Current epoch is 1, 89.25%\n",
            "Internal Progress: Current epoch is 1, 89.5%\n",
            "Internal Progress: Current epoch is 1, 89.75%\n",
            "Internal Progress: Current epoch is 1, 90.0%\n",
            "Internal Progress: Current epoch is 1, 90.25%\n",
            "Internal Progress: Current epoch is 1, 90.5%\n",
            "Internal Progress: Current epoch is 1, 90.75%\n",
            "Internal Progress: Current epoch is 1, 91.0%\n",
            "Internal Progress: Current epoch is 1, 91.25%\n",
            "Internal Progress: Current epoch is 1, 91.5%\n",
            "Internal Progress: Current epoch is 1, 91.75%\n",
            "Internal Progress: Current epoch is 1, 92.0%\n",
            "Internal Progress: Current epoch is 1, 92.25%\n",
            "Internal Progress: Current epoch is 1, 92.5%\n",
            "Internal Progress: Current epoch is 1, 92.75%\n",
            "Internal Progress: Current epoch is 1, 93.0%\n",
            "Internal Progress: Current epoch is 1, 93.25%\n",
            "Internal Progress: Current epoch is 1, 93.5%\n",
            "Internal Progress: Current epoch is 1, 93.75%\n",
            "Internal Progress: Current epoch is 1, 94.0%\n",
            "Internal Progress: Current epoch is 1, 94.25%\n",
            "Internal Progress: Current epoch is 1, 94.5%\n",
            "Internal Progress: Current epoch is 1, 94.75%\n",
            "Internal Progress: Current epoch is 1, 95.0%\n",
            "Internal Progress: Current epoch is 1, 95.25%\n",
            "Internal Progress: Current epoch is 1, 95.5%\n",
            "Internal Progress: Current epoch is 1, 95.75%\n",
            "Internal Progress: Current epoch is 1, 96.0%\n",
            "Internal Progress: Current epoch is 1, 96.25%\n",
            "Internal Progress: Current epoch is 1, 96.5%\n",
            "Internal Progress: Current epoch is 1, 96.75%\n",
            "Internal Progress: Current epoch is 1, 97.0%\n",
            "Internal Progress: Current epoch is 1, 97.25%\n",
            "Internal Progress: Current epoch is 1, 97.5%\n",
            "Internal Progress: Current epoch is 1, 97.75%\n",
            "Internal Progress: Current epoch is 1, 98.0%\n",
            "Internal Progress: Current epoch is 1, 98.25%\n",
            "Internal Progress: Current epoch is 1, 98.5%\n",
            "Internal Progress: Current epoch is 1, 98.75%\n",
            "Internal Progress: Current epoch is 1, 99.0%\n",
            "Internal Progress: Current epoch is 1, 99.25%\n",
            "Internal Progress: Current epoch is 1, 99.5%\n",
            "Internal Progress: Current epoch is 1, 99.75%\n"
          ]
        }
      ],
      "source": [
        "class CustomLSTM:\n",
        "    def __init__(self, input_size, hidden_size, learning_rate=0.001):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Parameters initialization\n",
        "        self.W_f = np.random.randn(hidden_size, input_size + hidden_size)  # Forget gate\n",
        "        self.b_f = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.W_i = np.random.randn(hidden_size, input_size + hidden_size)  # Input gate\n",
        "        self.b_i = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.W_c = np.random.randn(hidden_size, input_size + hidden_size)  # Cell state\n",
        "        self.b_c = np.zeros((hidden_size, 1))\n",
        "\n",
        "        self.W_o = np.random.randn(hidden_size, input_size + hidden_size)  # Output gate\n",
        "        self.b_o = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Cell state initialization\n",
        "        self.c = np.zeros((hidden_size, 1))\n",
        "\n",
        "        # Hidden state initialization\n",
        "        self.h = np.zeros((hidden_size, 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Adjust the dimensions of x\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.reshape(-1, 1)\n",
        "        #print(f'self.h is {self.h}')\n",
        "        #print(f' x is {x.values}')\n",
        "        concat_input = np.concatenate((self.h, np.array(x).reshape(len(x), 1)), axis=0)\n",
        "        #print(f'shape of concat is {concat_input.shape}')\n",
        "        # Forget gate\n",
        "        self.f = self.sigmoid(np.dot(self.W_f, concat_input) + self.b_f)\n",
        "\n",
        "        # Input gate\n",
        "        self.i = self.sigmoid(np.dot(self.W_i, concat_input) + self.b_i)\n",
        "\n",
        "        # Cell state\n",
        "        self.c_bar = self.tanh(np.dot(self.W_c, concat_input) + self.b_c)\n",
        "        self.c = self.f * self.c + self.i * self.c_bar\n",
        "\n",
        "        # Output gate\n",
        "        self.o = self.sigmoid(np.dot(self.W_o, concat_input) + self.b_o)\n",
        "\n",
        "        # Hidden state\n",
        "        self.h = self.o * self.tanh(self.c)\n",
        "\n",
        "        return self.h, self.c\n",
        "\n",
        "    def backward(self, x, dh_next, dc_next, d_next):\n",
        "        # Compute gradients of loss w.r.t. LSTM parameters\n",
        "        dW_f = np.zeros_like(self.W_f)\n",
        "        db_f = np.zeros_like(self.b_f)\n",
        "        dW_i = np.zeros_like(self.W_i)\n",
        "        db_i = np.zeros_like(self.b_i)\n",
        "        dW_c = np.zeros_like(self.W_c)\n",
        "        db_c = np.zeros_like(self.b_c)\n",
        "        dW_o = np.zeros_like(self.W_o)\n",
        "        db_o = np.zeros_like(self.b_o)\n",
        "\n",
        "        dh_prev = np.zeros_like(self.h)\n",
        "        dc_prev = np.zeros_like(self.c)\n",
        "\n",
        "        # Backpropagate through time\n",
        "        for t in reversed(range(len(x))):\n",
        "            concat_input = np.concatenate((self.h,np.array(x.reshape(-1, 1))), axis=0)\n",
        "            # Gradient from next hidden state and next cell state\n",
        "\n",
        "            d_concat = np.dot(self.W_f.T, d_next) + np.dot(self.W_i.T, d_next) + \\\n",
        "           np.dot(self.W_c.T, d_next) + np.dot(self.W_o.T, d_next)\n",
        "            dh = d_concat[:self.hidden_size, :]\n",
        "            #dc = dc_prev + d_concat[self.hidden_size:, :]\n",
        "            dc = dc_prev + dh\n",
        "            #dc = dc_prev + d_concat[self.hidden_size:, :].reshape(dc_prev.shape)\n",
        "            # Gradient for output gate\n",
        "            do = dh * self.tanh(self.c) * self.sigmoid(self.o) * (1 - self.sigmoid(self.o))\n",
        "            dW_o += np.dot(do, d_concat.T)\n",
        "            db_o += do\n",
        "\n",
        "            # Gradient for cell state\n",
        "            dc_bar = dc * self.i * (1 - self.c_bar ** 2)\n",
        "            dW_c += np.dot(dc_bar, d_concat.T)\n",
        "            db_c += dc_bar\n",
        "\n",
        "            # Gradient for input gate\n",
        "            di = dc * self.c_bar * self.sigmoid(self.i) * (1 - self.sigmoid(self.i))\n",
        "            dW_i += np.dot(di, d_concat.T)\n",
        "            db_i += di\n",
        "\n",
        "            # Gradient for forget gate\n",
        "            df = dc * self.c * self.sigmoid(self.f) * (1 - self.sigmoid(self.f))\n",
        "            dW_f += np.dot(df, d_concat.T)\n",
        "            db_f += df\n",
        "\n",
        "            # Gradient for previous hidden state and cell state\n",
        "            dh_prev = np.dot(self.W_f.T, df) + np.dot(self.W_i.T, di) + np.dot(self.W_c.T, dc_bar) + np.dot(self.W_o.T, do)\n",
        "            dc_prev = dc * self.f\n",
        "\n",
        "        # Update LSTM parameters\n",
        "        self.W_f -= self.learning_rate * dW_f\n",
        "        self.b_f -= self.learning_rate * db_f\n",
        "        self.W_i -= self.learning_rate * dW_i\n",
        "        self.b_i -= self.learning_rate * db_i\n",
        "        self.W_c -= self.learning_rate * dW_c\n",
        "        self.b_c -= self.learning_rate * db_c\n",
        "        self.W_o -= self.learning_rate * dW_o\n",
        "        self.b_o -= self.learning_rate * db_o\n",
        "\n",
        "        return dh_prev, dc_prev\n",
        "    def predict(self, X):\n",
        "        # Initialize hidden state and cell state\n",
        "        h_prev = np.zeros((self.hidden_size, 1))\n",
        "        c_prev = np.zeros((self.hidden_size, 1))\n",
        "\n",
        "        # Make predictions for each timestep in the input sequence\n",
        "        predicted_classes = []\n",
        "        for x in X:\n",
        "            # Forward pass through LSTM cell\n",
        "            h, c = self.forward(x)\n",
        "            print(f'h is {h}')\n",
        "            # Make prediction based on the current hidden state\n",
        "            # Pass hidden state through a fully connected layer with softmax activation\n",
        "            #output = np.dot(self.W_output, h) + self.b_output\n",
        "            probabilities = softmax(h)  # Apply softmax activation\n",
        "\n",
        "            # Predict the class with the highest probability\n",
        "            predicted_class = np.argmax(probabilities)\n",
        "\n",
        "            # Append the predicted class to the list of predicted classes\n",
        "            predicted_classes.append(predicted_class)\n",
        "\n",
        "        return predicted_classes\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_size = 5\n",
        "hidden_size = 4\n",
        "\n",
        "lstm = CustomLSTM(input_size, hidden_size)\n",
        "for epoch in range(2):\n",
        "    print(f'Progress: {epoch}%')\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for t in range(len(X_train)):\n",
        "        print(f'Internal Progress: Current epoch is {epoch}, {t/len(X_train) *100}%')\n",
        "        # Forward pass\n",
        "        h, c = lstm.forward(X_train.iloc[t])\n",
        "\n",
        "        # Calculate loss for this time step\n",
        "        loss = np.mean((h - y_train.iloc[t]) ** 2)\n",
        "        epoch_loss += loss\n",
        "\n",
        "        # Backward pass\n",
        "        dh_next = np.zeros_like(h)\n",
        "        dc_next = np.zeros_like(c)\n",
        "        d_next = 2.0 * (h - y_train.iloc[t])  # Gradient of mean squared error loss\n",
        "        lstm.backward(X_train[:t+1].values, dh_next, dc_next, d_next)\n",
        "\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Loss: {epoch_loss / len(X_train)}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "9xCuJQj28GDP",
        "outputId": "65343a86-9b21-48b7-c387-4cda12971182"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-7dbc8b7043a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-115-0966acabb5da>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Forward pass through LSTM cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;31m# Make prediction based on the current hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-115-0966acabb5da>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Adjust the dimensions of x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(f'self.h is {self.h}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjmsLMO6_tkZ"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x))  # Subtracting the maximum value for numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p80PMtaHZRS",
        "outputId": "338fc539-cff9-40a1-a86c-fb27690d8c41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Open      0.271357\n",
            "Close     0.275000\n",
            "High      0.275000\n",
            "Low       0.275000\n",
            "Volume    0.139883\n",
            "Name: 319, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.301508\n",
            "Close     0.305000\n",
            "High      0.310000\n",
            "Low       0.305000\n",
            "Volume    0.108322\n",
            "Name: 207, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.809045\n",
            "Close     0.810000\n",
            "High      0.815000\n",
            "Low       0.810000\n",
            "Volume    0.021298\n",
            "Name: 22, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.080402\n",
            "Close     0.085000\n",
            "High      0.085000\n",
            "Low       0.085000\n",
            "Volume    0.046885\n",
            "Name: 420, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.206030\n",
            "Close     0.210000\n",
            "High      0.210000\n",
            "Low       0.210000\n",
            "Volume    0.173276\n",
            "Name: 352, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.698493\n",
            "Close     0.715000\n",
            "High      0.745000\n",
            "Low       0.700000\n",
            "Volume    0.181420\n",
            "Name: 491, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.432161\n",
            "Close     0.425000\n",
            "High      0.430000\n",
            "Low       0.425000\n",
            "Volume    0.055269\n",
            "Name: 60, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.442211\n",
            "Close     0.445000\n",
            "High      0.445000\n",
            "Low       0.445000\n",
            "Volume    0.108611\n",
            "Name: 264, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.296482\n",
            "Close     0.300000\n",
            "High      0.305000\n",
            "Low       0.300000\n",
            "Volume    0.090541\n",
            "Name: 454, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.78392\n",
            "Close     0.78500\n",
            "High      0.78500\n",
            "Low       0.78500\n",
            "Volume    0.02178\n",
            "Name: 21, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.201005\n",
            "Close     0.205000\n",
            "High      0.205000\n",
            "Low       0.205000\n",
            "Volume    0.326122\n",
            "Name: 427, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.587940\n",
            "Close     0.580000\n",
            "High      0.585000\n",
            "Low       0.580000\n",
            "Volume    0.015034\n",
            "Name: 43, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.271357\n",
            "Close     0.270000\n",
            "High      0.270000\n",
            "Low       0.270000\n",
            "Volume    0.107310\n",
            "Name: 444, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.125628\n",
            "Close     0.130000\n",
            "High      0.130000\n",
            "Low       0.130000\n",
            "Volume    0.260059\n",
            "Name: 423, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.296482\n",
            "Close     0.295000\n",
            "High      0.295000\n",
            "Low       0.295000\n",
            "Volume    0.064858\n",
            "Name: 295, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.447236\n",
            "Close     0.450000\n",
            "High      0.450000\n",
            "Low       0.450000\n",
            "Volume    0.106828\n",
            "Name: 258, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.567839\n",
            "Close     0.570000\n",
            "High      0.570000\n",
            "Low       0.570000\n",
            "Volume    0.010842\n",
            "Name: 164, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.698493\n",
            "Close     0.700000\n",
            "High      0.700000\n",
            "Low       0.700000\n",
            "Volume    0.031899\n",
            "Name: 89, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.361809\n",
            "Close     0.365000\n",
            "High      0.365000\n",
            "Low       0.365000\n",
            "Volume    0.072857\n",
            "Name: 178, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.839196\n",
            "Close     0.835000\n",
            "High      0.835000\n",
            "Low       0.835000\n",
            "Volume    0.055751\n",
            "Name: 25, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.592965\n",
            "Close     0.585000\n",
            "High      0.590000\n",
            "Low       0.585000\n",
            "Volume    0.015998\n",
            "Name: 57, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.145729\n",
            "Close     0.150000\n",
            "High      0.150000\n",
            "Low       0.150000\n",
            "Volume    0.587770\n",
            "Name: 347, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.402010\n",
            "Close     0.395000\n",
            "High      0.400000\n",
            "Low       0.395000\n",
            "Volume    0.068135\n",
            "Name: 266, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.236181\n",
            "Close     0.240000\n",
            "High      0.245000\n",
            "Low       0.240000\n",
            "Volume    0.083747\n",
            "Name: 203, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.814070\n",
            "Close     0.815000\n",
            "High      0.830000\n",
            "Low       0.810000\n",
            "Volume    0.319472\n",
            "Name: 488, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.587940\n",
            "Close     0.590000\n",
            "High      0.595000\n",
            "Low       0.590000\n",
            "Volume    0.070496\n",
            "Name: 65, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.608040\n",
            "Close     0.605000\n",
            "High      0.605000\n",
            "Low       0.605000\n",
            "Volume    0.022358\n",
            "Name: 42, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.492462\n",
            "Close     0.490000\n",
            "High      0.490000\n",
            "Low       0.490000\n",
            "Volume    0.050161\n",
            "Name: 167, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.226131\n",
            "Close     0.220000\n",
            "High      0.225000\n",
            "Low       0.220000\n",
            "Volume    0.079844\n",
            "Name: 338, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.457286\n",
            "Close     0.460000\n",
            "High      0.460000\n",
            "Low       0.460000\n",
            "Volume    0.118489\n",
            "Name: 61, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.301508\n",
            "Close     0.300000\n",
            "High      0.300000\n",
            "Low       0.300000\n",
            "Volume    0.033007\n",
            "Name: 189, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.703518\n",
            "Close     0.790000\n",
            "High      0.785000\n",
            "Low       0.705000\n",
            "Volume    0.496844\n",
            "Name: 477, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.412060\n",
            "Close     0.415000\n",
            "High      0.415000\n",
            "Low       0.415000\n",
            "Volume    0.070689\n",
            "Name: 180, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.266332\n",
            "Close     0.270000\n",
            "High      0.270000\n",
            "Low       0.270000\n",
            "Volume    0.173469\n",
            "Name: 328, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.693467\n",
            "Close     0.695000\n",
            "High      0.700000\n",
            "Low       0.695000\n",
            "Volume    0.026647\n",
            "Name: 96, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.326633\n",
            "Close     0.330000\n",
            "High      0.330000\n",
            "Low       0.330000\n",
            "Volume    0.049872\n",
            "Name: 208, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.693467\n",
            "Close     0.690000\n",
            "High      0.690000\n",
            "Low       0.690000\n",
            "Volume    0.060280\n",
            "Name: 93, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.859296\n",
            "Close     0.860000\n",
            "High      0.865000\n",
            "Low       0.860000\n",
            "Volume    0.025249\n",
            "Name: 26, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.879397\n",
            "Close     0.880000\n",
            "High      0.880000\n",
            "Low       0.880000\n",
            "Volume    0.150195\n",
            "Name: 114, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.814070\n",
            "Close     0.795000\n",
            "High      0.820000\n",
            "Low       0.790000\n",
            "Volume    0.200357\n",
            "Name: 489, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.115578\n",
            "Close     0.120000\n",
            "High      0.120000\n",
            "Low       0.120000\n",
            "Volume    0.032333\n",
            "Name: 367, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.884422\n",
            "Close     0.885000\n",
            "High      0.885000\n",
            "Low       0.885000\n",
            "Volume    0.101287\n",
            "Name: 116, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.743719\n",
            "Close     0.745000\n",
            "High      0.745000\n",
            "Low       0.745000\n",
            "Volume    0.071411\n",
            "Name: 6, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.291457\n",
            "Close     0.295000\n",
            "High      0.295000\n",
            "Low       0.295000\n",
            "Volume    0.027080\n",
            "Name: 229, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.623116\n",
            "Close     0.625000\n",
            "High      0.625000\n",
            "Low       0.625000\n",
            "Volume    0.149039\n",
            "Name: 4, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.130653\n",
            "Close     0.135000\n",
            "High      0.135000\n",
            "Low       0.135000\n",
            "Volume    0.101961\n",
            "Name: 364, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.180905\n",
            "Close     0.180000\n",
            "High      0.180000\n",
            "Low       0.180000\n",
            "Volume    0.212644\n",
            "Name: 354, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.020100\n",
            "Close     0.025000\n",
            "High      0.025000\n",
            "Low       0.025000\n",
            "Volume    0.127403\n",
            "Name: 396, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.442211\n",
            "Close     0.445000\n",
            "High      0.445000\n",
            "Low       0.445000\n",
            "Volume    0.060280\n",
            "Name: 263, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.356784\n",
            "Close     0.360000\n",
            "High      0.360000\n",
            "Low       0.360000\n",
            "Volume    0.104467\n",
            "Name: 222, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.070352\n",
            "Close     0.070000\n",
            "High      0.070000\n",
            "Low       0.070000\n",
            "Volume    0.068087\n",
            "Name: 388, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.140703\n",
            "Close     0.145000\n",
            "High      0.145000\n",
            "Low       0.145000\n",
            "Volume    0.167928\n",
            "Name: 346, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.688442\n",
            "Close     0.685000\n",
            "High      0.685000\n",
            "Low       0.685000\n",
            "Volume    0.028767\n",
            "Name: 98, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.703518\n",
            "Close     0.705000\n",
            "High      0.710000\n",
            "Low       0.705000\n",
            "Volume    0.008095\n",
            "Name: 37, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.130653\n",
            "Close     0.135000\n",
            "High      0.135000\n",
            "Low       0.135000\n",
            "Volume    0.059847\n",
            "Name: 363, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.713568\n",
            "Close     0.755000\n",
            "High      0.760000\n",
            "Low       0.710000\n",
            "Volume    0.377777\n",
            "Name: 481, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.311558\n",
            "Close     0.315000\n",
            "High      0.315000\n",
            "Low       0.315000\n",
            "Volume    0.053727\n",
            "Name: 235, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.301508\n",
            "Close     0.300000\n",
            "High      0.300000\n",
            "Low       0.300000\n",
            "Volume    0.048330\n",
            "Name: 299, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.266332\n",
            "Close     0.265000\n",
            "High      0.265000\n",
            "Low       0.265000\n",
            "Volume    0.111406\n",
            "Name: 320, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.718593\n",
            "Close     0.715000\n",
            "High      0.730000\n",
            "Low       0.680000\n",
            "Volume    0.098492\n",
            "Name: 494, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.135678\n",
            "Close     0.135000\n",
            "High      0.135000\n",
            "Low       0.135000\n",
            "Volume    0.154966\n",
            "Name: 358, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.301508\n",
            "Close     0.300000\n",
            "High      0.300000\n",
            "Low       0.300000\n",
            "Volume    0.034983\n",
            "Name: 245, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.763819\n",
            "Close     0.715000\n",
            "High      0.770000\n",
            "Low       0.710000\n",
            "Volume    0.247434\n",
            "Name: 480, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.170854\n",
            "Close     0.175000\n",
            "High      0.175000\n",
            "Low       0.175000\n",
            "Volume    0.140943\n",
            "Name: 426, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.618090\n",
            "Close     0.620000\n",
            "High      0.625000\n",
            "Low       0.620000\n",
            "Volume    0.022792\n",
            "Name: 52, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.291457\n",
            "Close     0.295000\n",
            "High      0.300000\n",
            "Low       0.295000\n",
            "Volume    0.175011\n",
            "Name: 451, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.366834\n",
            "Close     0.365000\n",
            "High      0.365000\n",
            "Low       0.365000\n",
            "Volume    0.087216\n",
            "Name: 280, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.874372\n",
            "Close     0.875000\n",
            "High      0.875000\n",
            "Low       0.875000\n",
            "Volume    0.080470\n",
            "Name: 24, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.693467\n",
            "Close     0.695000\n",
            "High      0.700000\n",
            "Low       0.695000\n",
            "Volume    0.018166\n",
            "Name: 95, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.241206\n",
            "Close     0.245000\n",
            "High      0.245000\n",
            "Low       0.245000\n",
            "Volume    0.164073\n",
            "Name: 431, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.281407\n",
            "Close     0.285000\n",
            "High      0.290000\n",
            "Low       0.285000\n",
            "Volume    0.073098\n",
            "Name: 450, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.291457\n",
            "Close     0.290000\n",
            "High      0.290000\n",
            "Low       0.290000\n",
            "Volume    0.065340\n",
            "Name: 300, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.874372\n",
            "Close     0.870000\n",
            "High      0.875000\n",
            "Low       0.870000\n",
            "Volume    0.015179\n",
            "Name: 28, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.547739\n",
            "Close     0.540000\n",
            "High      0.545000\n",
            "Low       0.540000\n",
            "Volume    0.144750\n",
            "Name: 165, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.819095\n",
            "Close     0.820000\n",
            "High      0.825000\n",
            "Low       0.820000\n",
            "Volume    0.075893\n",
            "Name: 118, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.487437\n",
            "Close     0.490000\n",
            "High      0.490000\n",
            "Low       0.490000\n",
            "Volume    0.065147\n",
            "Name: 153, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.286432\n",
            "Close     0.290000\n",
            "High      0.290000\n",
            "Low       0.290000\n",
            "Volume    0.028526\n",
            "Name: 303, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.065327\n",
            "Close     0.070000\n",
            "High      0.070000\n",
            "Low       0.070000\n",
            "Volume    0.132559\n",
            "Name: 399, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.497487\n",
            "Close     0.520000\n",
            "High      0.525000\n",
            "Low       0.500000\n",
            "Volume    0.255288\n",
            "Name: 466, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.603015\n",
            "Close     0.600000\n",
            "High      0.600000\n",
            "Low       0.600000\n",
            "Volume    0.020527\n",
            "Name: 55, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.678392\n",
            "Close     0.680000\n",
            "High      0.680000\n",
            "Low       0.680000\n",
            "Volume    0.008047\n",
            "Name: 101, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.090452\n",
            "Close     0.095000\n",
            "High      0.095000\n",
            "Low       0.095000\n",
            "Volume    0.171252\n",
            "Name: 401, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.40201\n",
            "Close     0.40500\n",
            "High      0.40500\n",
            "Low       0.40500\n",
            "Volume    0.10172\n",
            "Name: 255, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.316583\n",
            "Close     0.310000\n",
            "High      0.315000\n",
            "Low       0.310000\n",
            "Volume    0.099937\n",
            "Name: 248, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.793970\n",
            "Close     0.685000\n",
            "High      0.790000\n",
            "Low       0.685000\n",
            "Volume    0.208837\n",
            "Name: 490, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.723618\n",
            "Close     0.715000\n",
            "High      0.720000\n",
            "Low       0.715000\n",
            "Volume    0.040572\n",
            "Name: 133, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.010050\n",
            "Close     0.015000\n",
            "High      0.015000\n",
            "Low       0.015000\n",
            "Volume    0.267287\n",
            "Name: 395, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.788945\n",
            "Close     0.765000\n",
            "High      0.785000\n",
            "Low       0.745000\n",
            "Volume    0.295427\n",
            "Name: 479, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.311558\n",
            "Close     0.315000\n",
            "High      0.315000\n",
            "Low       0.315000\n",
            "Volume    0.077097\n",
            "Name: 236, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.221106\n",
            "Close     0.225000\n",
            "High      0.225000\n",
            "Low       0.225000\n",
            "Volume    0.132607\n",
            "Name: 318, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.371859\n",
            "Close     0.365000\n",
            "High      0.370000\n",
            "Low       0.365000\n",
            "Volume    0.073917\n",
            "Name: 285, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.316583\n",
            "Close     0.320000\n",
            "High      0.320000\n",
            "Low       0.320000\n",
            "Volume    0.286079\n",
            "Name: 246, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.216080\n",
            "Close     0.220000\n",
            "High      0.220000\n",
            "Low       0.220000\n",
            "Volume    0.103985\n",
            "Name: 310, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.246231\n",
            "Close     0.250000\n",
            "High      0.250000\n",
            "Low       0.250000\n",
            "Volume    0.759167\n",
            "Name: 428, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.587940\n",
            "Close     0.585000\n",
            "High      0.585000\n",
            "Low       0.585000\n",
            "Volume    0.039031\n",
            "Name: 46, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.432161\n",
            "Close     0.500000\n",
            "High      0.500000\n",
            "Low       0.430000\n",
            "Volume    0.583771\n",
            "Name: 459, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.266332\n",
            "Close     0.265000\n",
            "High      0.265000\n",
            "Low       0.265000\n",
            "Volume    0.127837\n",
            "Name: 438, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.276382\n",
            "Close     0.275000\n",
            "High      0.275000\n",
            "Low       0.275000\n",
            "Volume    0.080856\n",
            "Name: 272, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.693467\n",
            "Close     0.685000\n",
            "High      0.690000\n",
            "Low       0.685000\n",
            "Volume    0.013829\n",
            "Name: 136, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n",
            "Open      0.758794\n",
            "Close     0.755000\n",
            "High      0.755000\n",
            "Low       0.755000\n",
            "Volume    0.085482\n",
            "Name: 32, dtype: float64\n",
            "[[nan]\n",
            " [nan]\n",
            " [nan]\n",
            " [nan]]\n"
          ]
        }
      ],
      "source": [
        "#predictions = lstm.predict(X_val)\n",
        "for i in range(len(X_val)):\n",
        "  print(X_val.iloc[i])\n",
        "  #print(i)\n",
        "  h_prev = np.zeros((4, 1))\n",
        "  c_prev = np.zeros((4, 1))\n",
        "\n",
        "  # Make predictions for each timestep in the input sequence\n",
        "  predicted_classes = []\n",
        "      # Forward pass through LSTM cell\n",
        "  h, c = lstm.forward(X_val.iloc[i])\n",
        "  print(c)\n",
        "  #print(f'h.shape is {h.shape}')\n",
        "  #print(f'lstm Wo is {lstm.W_o.shape}')\n",
        "  #print(f' bo is {lstm.b_o.shape}')\n",
        "      # Make prediction based on the current hidden state\n",
        "      # Pass hidden state through a fully connected layer with softmax activation\n",
        "  #output = np.dot(h.T,lstm.W_o).T + lstm.b_o\n",
        "  probabilities = softmax(h)  # Apply softmax activation\n",
        "\n",
        "      # Predict the class with the highest probability\n",
        "predicted_class = np.argmax(probabilities)\n",
        "\n",
        "      # Append the predicted class to the list of predicted classes\n",
        "predicted_classes.append(predicted_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBgMgLIf-_Qa",
        "outputId": "5cac6639-c0e2-4220-f813-ab27eba949e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5,)"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_val.iloc[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xitM1Wxu_C1S",
        "outputId": "3385178d-f1e6-4657-c0d9-6b55a1c7991c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frDpGFEuCzBb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Vvy7Pidmfz"
      },
      "source": [
        "#New Implementation with Pytorch Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "XEWRVA9uM4W0",
        "outputId": "c867e81a-05c4-47d4-a960-530964ad20a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 400,\n  \"fields\": [\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23738218527428884,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 153,\n        \"samples\": [\n          0.3618090491033451,\n          0.42713570300384274,\n          0.21608042156469698\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23650281682494606,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 154,\n        \"samples\": [\n          0.8500000317096725,\n          0.285000026535989,\n          0.025000005841255446\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23610846341554637,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 154,\n        \"samples\": [\n          0.8500000367164617,\n          0.34500002236366295,\n          0.05500002436637902\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.23610846341554637,\n        \"min\": 0.0,\n        \"max\": 1.0000000000000002,\n        \"num_unique_values\": 154,\n        \"samples\": [\n          0.8500000367164617,\n          0.34500002236366295,\n          0.05500002436637902\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Adj Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08351700212378757,\n        \"min\": 0.1556382328271865,\n        \"max\": 0.509361207485199,\n        \"num_unique_values\": 154,\n        \"samples\": [\n          0.4563027024269104,\n          0.277672678232193,\n          0.1750929206609726\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0821978951093079,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 375,\n        \"samples\": [\n          0.07859104707753096,\n          0.0421625789042548,\n          0.08818002216547005\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cat_encoded\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0,\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-f1d8bcd5-728a-4985-a608-f1f67e84d863\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Cat_encoded</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.708543</td>\n",
              "      <td>0.710</td>\n",
              "      <td>0.710</td>\n",
              "      <td>0.710</td>\n",
              "      <td>0.406782</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.653266</td>\n",
              "      <td>0.650</td>\n",
              "      <td>0.650</td>\n",
              "      <td>0.650</td>\n",
              "      <td>0.385558</td>\n",
              "      <td>0.369392</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.572864</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.357260</td>\n",
              "      <td>0.218474</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.592965</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.595</td>\n",
              "      <td>0.366103</td>\n",
              "      <td>0.176986</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.623116</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.376715</td>\n",
              "      <td>0.149039</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>0.010050</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.015</td>\n",
              "      <td>0.160944</td>\n",
              "      <td>0.267287</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>0.020100</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.164481</td>\n",
              "      <td>0.127403</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>0.050251</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.055</td>\n",
              "      <td>0.175093</td>\n",
              "      <td>0.237074</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>0.055276</td>\n",
              "      <td>0.065</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.060</td>\n",
              "      <td>0.176862</td>\n",
              "      <td>0.144027</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>0.065327</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.070</td>\n",
              "      <td>0.180399</td>\n",
              "      <td>0.132559</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1d8bcd5-728a-4985-a608-f1f67e84d863')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f1d8bcd5-728a-4985-a608-f1f67e84d863 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f1d8bcd5-728a-4985-a608-f1f67e84d863');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7316a8c5-ffe2-4f6d-b127-0ea205471675\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7316a8c5-ffe2-4f6d-b127-0ea205471675')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7316a8c5-ffe2-4f6d-b127-0ea205471675 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6588f35c-f5b7-44b7-951d-7092ee2e9779\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6588f35c-f5b7-44b7-951d-7092ee2e9779 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "         Open   High    Low  Close  Adj Close    Volume  Cat_encoded\n",
              "0    0.708543  0.710  0.710  0.710   0.406782  1.000000            1\n",
              "1    0.653266  0.650  0.650  0.650   0.385558  0.369392            1\n",
              "2    0.572864  0.570  0.570  0.570   0.357260  0.218474            0\n",
              "3    0.592965  0.595  0.595  0.595   0.366103  0.176986            2\n",
              "4    0.623116  0.625  0.625  0.625   0.376715  0.149039            2\n",
              "..        ...    ...    ...    ...        ...       ...          ...\n",
              "395  0.010050  0.015  0.015  0.015   0.160944  0.267287            2\n",
              "396  0.020100  0.025  0.025  0.025   0.164481  0.127403            2\n",
              "397  0.050251  0.055  0.055  0.055   0.175093  0.237074            3\n",
              "398  0.055276  0.065  0.060  0.060   0.176862  0.144027            2\n",
              "399  0.065327  0.070  0.070  0.070   0.180399  0.132559            2\n",
              "\n",
              "[400 rows x 7 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data2 = data.copy(deep = True).drop(columns = ['%Change','ChgCat','Binary'])\n",
        "train = data2[:400]\n",
        "test = data2[400:]\n",
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNVflJ_seVeg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LSTM_cell(torch.nn.Module):\n",
        "    def __init__(self, input_length=10, hidden_length=20):\n",
        "        super(LSTM_cell, self).__init__()\n",
        "        self.input_length = input_length\n",
        "        self.hidden_length = hidden_length\n",
        "\n",
        "        # forget gate components\n",
        "        self.forget_w = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.forget_r = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_forget = nn.Sigmoid()\n",
        "\n",
        "        # input gate components\n",
        "        self.input_w = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.input_r = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_gate = nn.Sigmoid()\n",
        "\n",
        "        # cell memory components\n",
        "        self.memory_w = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.memory_r = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.activation_gate = nn.Tanh()\n",
        "\n",
        "        # out gate components\n",
        "        self.output_w = nn.Linear(self.input_length, self.hidden_length, bias=True)\n",
        "        self.output_r = nn.Linear(self.hidden_length, self.hidden_length, bias=False)\n",
        "        self.sigmoid_hidden_out = nn.Sigmoid()\n",
        "\n",
        "        self.activation_final = nn.Tanh()\n",
        "\n",
        "    def forget(self, x, h):\n",
        "        x = self.forget_w(x)\n",
        "        h = self.forget_r(h)\n",
        "        return self.sigmoid_forget(x + h)\n",
        "\n",
        "    def input_gate(self, x, h):\n",
        "\n",
        "        x_temp = self.input_w(x)\n",
        "        h_temp = self.input_r(h)\n",
        "        i = self.sigmoid_gate(x_temp + h_temp)\n",
        "        return i\n",
        "\n",
        "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
        "        x = self.memory_w(x)\n",
        "        h = self.memory_r(h)\n",
        "\n",
        "        # new information part that will be injected in the new context\n",
        "        k = self.activation_gate(x + h)\n",
        "        g = k * i\n",
        "\n",
        "        # forget old context/cell info\n",
        "        c = f * c_prev\n",
        "        # learn new context/cell info\n",
        "        c_next = g + c\n",
        "        return c_next\n",
        "\n",
        "    def out_gate(self, x, h):\n",
        "        x = self.output_w(x)\n",
        "        h = self.output_r(h)\n",
        "        return self.sigmoid_hidden_out(x + h)\n",
        "\n",
        "    # Forward function defining the flow of data through the LSTM cell\n",
        "    def forward(self, x, tuple_in ):\n",
        "        (h, c_prev) = tuple_in\n",
        "        # Input Gate\n",
        "        i = self.input_gate(x, h)\n",
        "\n",
        "        # Forget Gate\n",
        "        f = self.forget(x, h)\n",
        "\n",
        "        # Update cell memory\n",
        "        c_next = self.cell_memory_gate(i, f, x, h,c_prev)\n",
        "\n",
        "        # Output Gate\n",
        "        o = self.out_gate(x, h)\n",
        "\n",
        "        # Hidden State Output\n",
        "        h_next = o * self.activation_final(c_next)\n",
        "        return h_next, c_next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV1gTwLJjGqP"
      },
      "outputs": [],
      "source": [
        "class Sequence(nn.Module):\n",
        "    def __init__(self,no_classes = 5):\n",
        "        super(Sequence, self).__init__()\n",
        "        # # Define two LSTM cells with input and hidden lengths\n",
        "        self.rnn1 = LSTM_cell(1, 5)\n",
        "        self.rnn2 = LSTM_cell(5, 5)\n",
        "        # Linear class for number of classes at the end\n",
        "        self.linear = nn.Linear(5, no_classes)\n",
        "\n",
        "\n",
        "    # Forward function defining the flow of data through the network\n",
        "    def forward(self, input, future=0):\n",
        "        outputs = []\n",
        "        # Initialise hidden and cell states for both LSTM cells\n",
        "        h_t = torch.zeros(input.size(0), 5, dtype=torch.double)\n",
        "        c_t = torch.zeros(input.size(0), 5, dtype=torch.double)\n",
        "        h_t2 = torch.zeros(input.size(0), 5, dtype=torch.double)\n",
        "        c_t2 = torch.zeros(input.size(0), 5, dtype=torch.double)\n",
        "\n",
        "        # Iterate through each time step of the input sequence\n",
        "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
        "            h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
        "            h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
        "            output = self.linear(h_t2)\n",
        "            outputs += [output]\n",
        "\n",
        "        for i in range(future):\n",
        "            h_t, c_t = self.rnn1(input_t, (h_t, c_t))\n",
        "            h_t2, c_t2 = self.rnn2(h_t, (h_t2, c_t2))\n",
        "            output = self.linear(h_t2)\n",
        "            outputs += [output]\n",
        "        outputs = torch.stack(outputs, 1).squeeze(2)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PbVEMQJdwQ5",
        "outputId": "3709147d-f77f-4932-ee65-1c5be36a05e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STEP:  0\n",
            "Step: 0, loss: 1.807376254866498\n",
            "Step: 0, loss: 1.8070407220674312\n",
            "Step: 0, loss: 1.7981752010432286\n",
            "Step: 0, loss: 1.7692629440705567\n",
            "Step: 0, loss: 1.745480304837435\n",
            "Step: 0, loss: 1.7249473661063706\n",
            "Step: 0, loss: 1.6142775524024995\n",
            "Step: 0, loss: 1.4266623215913548\n",
            "Step: 0, loss: 1.1393545495576918\n",
            "Step: 0, loss: 0.6772128350018373\n",
            "Step: 0, loss: 0.6483418092098957\n",
            "Step: 0, loss: 0.6314230988342167\n",
            "Step: 0, loss: 0.6144530840496973\n",
            "Step: 0, loss: 0.5974724843084658\n",
            "Step: 0, loss: 0.5812444914335296\n",
            "Step: 0, loss: 0.5668169421451925\n",
            "Step: 0, loss: 0.5543773403605698\n",
            "Step: 0, loss: 0.5431162557412894\n",
            "Step: 0, loss: 0.5318166450959837\n",
            "Step: 0, loss: 0.5192066169294935\n",
            "STEP:  1\n",
            "Step: 1, loss: 0.5053227841542446\n",
            "Step: 1, loss: 0.49319930924138694\n",
            "Step: 1, loss: 0.4818460344628218\n",
            "Step: 1, loss: 0.4692017080933362\n",
            "Step: 1, loss: 0.45376788326386136\n",
            "Step: 1, loss: 0.43378426952879495\n",
            "Step: 1, loss: 0.4160952671549602\n",
            "Step: 1, loss: 0.40599530944188505\n",
            "Step: 1, loss: 0.39581506860887167\n",
            "Step: 1, loss: 0.3890184421576339\n",
            "Step: 1, loss: 0.3839413990255229\n",
            "Step: 1, loss: 0.3796642931514274\n",
            "Step: 1, loss: 0.37618802088715425\n",
            "Step: 1, loss: 0.3733023495714088\n",
            "Step: 1, loss: 0.3708826061085037\n",
            "Step: 1, loss: 0.3688248242661561\n",
            "Step: 1, loss: 0.3670573141050382\n",
            "Step: 1, loss: 0.36552466290470415\n",
            "Step: 1, loss: 0.3641797380389122\n",
            "Step: 1, loss: 0.36299204112526645\n",
            "STEP:  2\n",
            "Step: 2, loss: 0.36186478021558843\n",
            "Step: 2, loss: 0.3607349044520192\n",
            "Step: 2, loss: 0.3598954805821612\n",
            "Step: 2, loss: 0.35914409429356253\n",
            "Step: 2, loss: 0.35845085088072876\n",
            "Step: 2, loss: 0.3577451797229033\n",
            "Step: 2, loss: 0.35706174419601183\n",
            "Step: 2, loss: 0.3564602747684476\n",
            "Step: 2, loss: 0.35593251517282454\n",
            "Step: 2, loss: 0.35544371325443\n",
            "Step: 2, loss: 0.35501083384737137\n",
            "Step: 2, loss: 0.35461051671383237\n",
            "Step: 2, loss: 0.35425694416541176\n",
            "Step: 2, loss: 0.35393190230177896\n",
            "Step: 2, loss: 0.353646331227787\n",
            "Step: 2, loss: 0.35338548067638126\n",
            "Step: 2, loss: 0.3531571104732553\n",
            "Step: 2, loss: 0.3529493601257169\n",
            "Step: 2, loss: 0.35276731915582393\n",
            "Step: 2, loss: 0.352601691447989\n",
            "STEP:  3\n",
            "Step: 3, loss: 0.35245573476503084\n",
            "Step: 3, loss: 0.35232237349417395\n",
            "Step: 3, loss: 0.3522037905615062\n",
            "Step: 3, loss: 0.3520947033777397\n",
            "Step: 3, loss: 0.35199670248923554\n",
            "Step: 3, loss: 0.35190582750131816\n",
            "Step: 3, loss: 0.35182333809094424\n",
            "Step: 3, loss: 0.3517461725019907\n",
            "Step: 3, loss: 0.3516754051791843\n",
            "Step: 3, loss: 0.3516085425010493\n",
            "Step: 3, loss: 0.35154656352554986\n",
            "Step: 3, loss: 0.3514872944792004\n",
            "Step: 3, loss: 0.3514316800107263\n",
            "Step: 3, loss: 0.3513776655021156\n",
            "Step: 3, loss: 0.3513262111314578\n",
            "Step: 3, loss: 0.35127518344415526\n",
            "Step: 3, loss: 0.3512256133642257\n",
            "Step: 3, loss: 0.3511750241616473\n",
            "Step: 3, loss: 0.35112459985062433\n",
            "Step: 3, loss: 0.35107105957342183\n",
            "STEP:  4\n",
            "Step: 4, loss: 0.35101592456773284\n",
            "Step: 4, loss: 0.3509542535440932\n",
            "Step: 4, loss: 0.3508886886048279\n",
            "Step: 4, loss: 0.3508120491634359\n",
            "Step: 4, loss: 0.35073308328042824\n",
            "Step: 4, loss: 0.35064876677719764\n",
            "Step: 4, loss: 0.35058293370677224\n",
            "Step: 4, loss: 0.3505299146645942\n",
            "Step: 4, loss: 0.3504932801088949\n",
            "Step: 4, loss: 0.35046636960548766\n",
            "Step: 4, loss: 0.3504461399605146\n",
            "Step: 4, loss: 0.35042894499585464\n",
            "Step: 4, loss: 0.35041406324218105\n",
            "Step: 4, loss: 0.35039901883140306\n",
            "Step: 4, loss: 0.35038289454570276\n",
            "Step: 4, loss: 0.35036183730489834\n",
            "Step: 4, loss: 0.35033199165705603\n",
            "Step: 4, loss: 0.35029106955726924\n",
            "Step: 4, loss: 0.35023416985411454\n",
            "Step: 4, loss: 0.3501765545675468\n",
            "STEP:  5\n",
            "Step: 5, loss: 0.3501031536652875\n",
            "Step: 5, loss: 0.35001603399152753\n",
            "Step: 5, loss: 0.3499308178607711\n",
            "Step: 5, loss: 0.3498547183966405\n",
            "Step: 5, loss: 0.3497948337975604\n",
            "Step: 5, loss: 0.3497389572694107\n",
            "Step: 5, loss: 0.3497045179939684\n",
            "Step: 5, loss: 0.3496743100358249\n",
            "Step: 5, loss: 0.34965142014539496\n",
            "Step: 5, loss: 0.3496306750143908\n",
            "Step: 5, loss: 0.34961380002693876\n",
            "Step: 5, loss: 0.34959810432977195\n",
            "Step: 5, loss: 0.3495850159931308\n",
            "Step: 5, loss: 0.34957316136656363\n",
            "Step: 5, loss: 0.3495629059578645\n",
            "Step: 5, loss: 0.3495536662620314\n",
            "Step: 5, loss: 0.3495456545751389\n",
            "Step: 5, loss: 0.3495382433559902\n",
            "Step: 5, loss: 0.34953148149203833\n",
            "Step: 5, loss: 0.34952492254487627\n",
            "STEP:  6\n",
            "Step: 6, loss: 0.34951844323948805\n",
            "Step: 6, loss: 0.34951205644435157\n",
            "Step: 6, loss: 0.3495055977440156\n",
            "Step: 6, loss: 0.3494992419833968\n",
            "Step: 6, loss: 0.34949274419292514\n",
            "Step: 6, loss: 0.34948635115132337\n",
            "Step: 6, loss: 0.3494796671022906\n",
            "Step: 6, loss: 0.3494727430441733\n",
            "Step: 6, loss: 0.3494652252870654\n",
            "Step: 6, loss: 0.3494572730590724\n",
            "Step: 6, loss: 0.3494488598113976\n",
            "Step: 6, loss: 0.349439904935143\n",
            "Step: 6, loss: 0.34943109447167714\n",
            "Step: 6, loss: 0.3494219508218224\n",
            "Step: 6, loss: 0.34941262517343236\n",
            "Step: 6, loss: 0.3494041523884262\n",
            "Step: 6, loss: 0.3493959994167053\n",
            "Step: 6, loss: 0.3493889263628786\n",
            "Step: 6, loss: 0.34938199071434145\n",
            "Step: 6, loss: 0.34937446700762603\n",
            "STEP:  7\n",
            "Step: 7, loss: 0.3493664826708846\n",
            "Step: 7, loss: 0.3493580672604681\n",
            "Step: 7, loss: 0.34934882124803746\n",
            "Step: 7, loss: 0.34933650488130696\n",
            "Step: 7, loss: 0.3493220985687805\n",
            "Step: 7, loss: 0.3493051995833309\n",
            "Step: 7, loss: 0.34928418784960896\n",
            "Step: 7, loss: 0.3492583662153097\n",
            "Step: 7, loss: 0.3492270251325914\n",
            "Step: 7, loss: 0.34919499341851873\n",
            "Step: 7, loss: 0.34917162355131803\n",
            "Step: 7, loss: 0.3491535845073438\n",
            "Step: 7, loss: 0.3491387226195056\n",
            "Step: 7, loss: 0.34912152102200306\n",
            "Step: 7, loss: 0.3491068960550175\n",
            "Step: 7, loss: 0.3490926302130661\n",
            "Step: 7, loss: 0.3490808226997241\n",
            "Step: 7, loss: 0.34906940179285284\n",
            "Step: 7, loss: 0.3490577968870613\n",
            "Step: 7, loss: 0.34904585011275285\n",
            "STEP:  8\n",
            "Step: 8, loss: 0.3490340659243382\n",
            "Step: 8, loss: 0.349021721529892\n",
            "Step: 8, loss: 0.34900944345703605\n",
            "Step: 8, loss: 0.3489965688938409\n",
            "Step: 8, loss: 0.3489862089343929\n",
            "Step: 8, loss: 0.34897627282677224\n",
            "Step: 8, loss: 0.3489672635456594\n",
            "Step: 8, loss: 0.34895906707119667\n",
            "Step: 8, loss: 0.34895266839991546\n",
            "Step: 8, loss: 0.34894722926870364\n",
            "Step: 8, loss: 0.3489429057800749\n",
            "Step: 8, loss: 0.34893910786742\n",
            "Step: 8, loss: 0.3489358562824567\n",
            "Step: 8, loss: 0.3489328768442949\n",
            "Step: 8, loss: 0.34893015718327564\n",
            "Step: 8, loss: 0.3489275096635613\n",
            "Step: 8, loss: 0.3489248784062703\n",
            "Step: 8, loss: 0.34892213746669226\n",
            "Step: 8, loss: 0.34891919246834624\n",
            "Step: 8, loss: 0.3489159971880099\n",
            "STEP:  9\n",
            "Step: 9, loss: 0.34891248274789566\n",
            "Step: 9, loss: 0.3489086549970682\n",
            "Step: 9, loss: 0.3489044891230909\n",
            "Step: 9, loss: 0.3488999958962878\n",
            "Step: 9, loss: 0.34889514090920243\n",
            "Step: 9, loss: 0.34889006776078896\n",
            "Step: 9, loss: 0.34888480127287286\n",
            "Step: 9, loss: 0.34887954320520737\n",
            "Step: 9, loss: 0.34887424873422346\n",
            "Step: 9, loss: 0.34886922648039526\n",
            "Step: 9, loss: 0.3488643499931401\n",
            "Step: 9, loss: 0.34885994048083874\n",
            "Step: 9, loss: 0.34885578081172836\n",
            "Step: 9, loss: 0.3488521152607241\n",
            "Step: 9, loss: 0.3488486968990121\n",
            "Step: 9, loss: 0.3488456566731661\n",
            "Step: 9, loss: 0.3488427930491786\n",
            "Step: 9, loss: 0.348840144275872\n",
            "Step: 9, loss: 0.3488375372286775\n",
            "Step: 9, loss: 0.3488349614685669\n",
            "STEP:  10\n",
            "Step: 10, loss: 0.34883225142039764\n",
            "Step: 10, loss: 0.34882936015739047\n",
            "Step: 10, loss: 0.3488260647271372\n",
            "Step: 10, loss: 0.3488222527637068\n",
            "Step: 10, loss: 0.34881765430755285\n",
            "Step: 10, loss: 0.34881195171505547\n",
            "Step: 10, loss: 0.348804415090866\n",
            "Step: 10, loss: 0.3487987634227784\n",
            "Step: 10, loss: 0.3487917498477084\n",
            "Step: 10, loss: 0.34878257216623454\n",
            "Step: 10, loss: 0.34877340189293665\n",
            "Step: 10, loss: 0.34876532667817656\n",
            "Step: 10, loss: 0.3487593788672305\n",
            "Step: 10, loss: 0.3487544620785461\n",
            "Step: 10, loss: 0.3487504258963847\n",
            "Step: 10, loss: 0.34874676384853204\n",
            "Step: 10, loss: 0.3487434834268158\n",
            "Step: 10, loss: 0.34874031229137864\n",
            "Step: 10, loss: 0.34873730200111674\n",
            "Step: 10, loss: 0.34873429447942045\n",
            "STEP:  11\n",
            "Step: 11, loss: 0.34873135309907294\n",
            "Step: 11, loss: 0.34872838975325615\n",
            "Step: 11, loss: 0.3487254635730997\n",
            "Step: 11, loss: 0.34872256992697154\n",
            "Step: 11, loss: 0.3487196284188459\n",
            "Step: 11, loss: 0.34871674115640666\n",
            "Step: 11, loss: 0.3487137215771531\n",
            "Step: 11, loss: 0.34870958204946184\n",
            "Step: 11, loss: 0.34870680359009987\n",
            "Step: 11, loss: 0.34870398930800395\n",
            "Step: 11, loss: 0.34870045828486923\n",
            "Step: 11, loss: 0.348696374870016\n",
            "Step: 11, loss: 0.3486916016123731\n",
            "Step: 11, loss: 0.3486872359193035\n",
            "Step: 11, loss: 0.34868227988630496\n",
            "Step: 11, loss: 0.3486769972984553\n",
            "Step: 11, loss: 0.34867047512697225\n",
            "Step: 11, loss: 0.34866323292993395\n",
            "Step: 11, loss: 0.3486540713447861\n",
            "Step: 11, loss: 0.3486438533178928\n",
            "STEP:  12\n",
            "Step: 12, loss: 0.34863034455889447\n",
            "Step: 12, loss: 0.34861080894273305\n",
            "Step: 12, loss: 0.34859362508338076\n",
            "Step: 12, loss: 0.3485697113047542\n",
            "Step: 12, loss: 0.34853292519719337\n",
            "Step: 12, loss: 0.3484934072358046\n",
            "Step: 12, loss: 0.34847463369634624\n",
            "Step: 12, loss: 0.3484531915162043\n",
            "Step: 12, loss: 0.3484210844947031\n",
            "Step: 12, loss: 0.34840376488272357\n",
            "Step: 12, loss: 0.34838871482282974\n",
            "Step: 12, loss: 0.34837748821904646\n",
            "Step: 12, loss: 0.3483677974986511\n",
            "Step: 12, loss: 0.348358199731602\n",
            "Step: 12, loss: 0.34835019849191196\n",
            "Step: 12, loss: 0.34834286340275156\n",
            "Step: 12, loss: 0.34833689114129474\n",
            "Step: 12, loss: 0.348330137215858\n",
            "Step: 12, loss: 0.3483223564711809\n",
            "Step: 12, loss: 0.3483139392441951\n",
            "STEP:  13\n",
            "Step: 13, loss: 0.34830507970556623\n",
            "Step: 13, loss: 0.3482951837180722\n",
            "Step: 13, loss: 0.3482838931823237\n",
            "Step: 13, loss: 0.34826942225473\n",
            "Step: 13, loss: 0.34825256847687286\n",
            "Step: 13, loss: 0.3482264737526396\n",
            "Step: 13, loss: 0.3482120804864247\n",
            "Step: 13, loss: 0.34819137993976407\n",
            "Step: 13, loss: 0.3481771982276037\n",
            "Step: 13, loss: 0.34815843966436355\n",
            "Step: 13, loss: 0.3481455195963902\n",
            "Step: 13, loss: 0.3481297839913698\n",
            "Step: 13, loss: 0.3481151543627379\n",
            "Step: 13, loss: 0.348099899908173\n",
            "Step: 13, loss: 0.34808133232801514\n",
            "Step: 13, loss: 0.3480666092405969\n",
            "Step: 13, loss: 0.34805513040200814\n",
            "Step: 13, loss: 0.3480450417229846\n",
            "Step: 13, loss: 0.34803220844504057\n",
            "Step: 13, loss: 0.3480224890636518\n",
            "STEP:  14\n",
            "Step: 14, loss: 0.34801358742093397\n",
            "Step: 14, loss: 0.34800490845505466\n",
            "Step: 14, loss: 0.34799636107027737\n",
            "Step: 14, loss: 0.347985690992548\n",
            "Step: 14, loss: 0.347972783086675\n",
            "Step: 14, loss: 0.34795673889869816\n",
            "Step: 14, loss: 0.34794085433585437\n",
            "Step: 14, loss: 0.34792534055257013\n",
            "Step: 14, loss: 0.34790904453484106\n",
            "Step: 14, loss: 0.347886281594623\n",
            "Step: 14, loss: 0.3478625194959136\n",
            "Step: 14, loss: 0.3478343689493916\n",
            "Step: 14, loss: 0.34780141093137834\n",
            "Step: 14, loss: 0.347764200758519\n",
            "Step: 14, loss: 0.3477218988847474\n",
            "Step: 14, loss: 0.3476779294937119\n",
            "Step: 14, loss: 0.3476372557841647\n",
            "Step: 14, loss: 0.3475817722565858\n",
            "Step: 14, loss: 0.34755840039780855\n",
            "Step: 14, loss: 0.34752990714191867\n",
            "STEP:  15\n",
            "Step: 15, loss: 0.34750051871334187\n",
            "Step: 15, loss: 0.3474640910959389\n",
            "Step: 15, loss: 0.34741727980792303\n",
            "Step: 15, loss: 0.34738586750406997\n",
            "Step: 15, loss: 0.3473392280901794\n",
            "Step: 15, loss: 0.34728557780383923\n",
            "Step: 15, loss: 0.34723550034985445\n",
            "Step: 15, loss: 0.3471589199847773\n",
            "Step: 15, loss: 0.34709168024945414\n",
            "Step: 15, loss: 0.3469854083258198\n",
            "Step: 15, loss: 0.34690829060254785\n",
            "Step: 15, loss: 0.34682401808714375\n",
            "Step: 15, loss: 0.3467381274985077\n",
            "Step: 15, loss: 0.3466670858212535\n",
            "Step: 15, loss: 0.34656468514793176\n",
            "Step: 15, loss: 0.34648446066002947\n",
            "Step: 15, loss: 0.3463922730698061\n",
            "Step: 15, loss: 0.34632884148770915\n",
            "Step: 15, loss: 0.34625557551998054\n",
            "Step: 15, loss: 0.3461949188929906\n",
            "STEP:  16\n",
            "Step: 16, loss: 0.3461399664049311\n",
            "Step: 16, loss: 0.3460878046469536\n",
            "Step: 16, loss: 0.34603851965367294\n",
            "Step: 16, loss: 0.3459779751720768\n",
            "Step: 16, loss: 0.3459306774954957\n",
            "Step: 16, loss: 0.3458893288315628\n",
            "Step: 16, loss: 0.3458554055741253\n",
            "Step: 16, loss: 0.34581753641248136\n",
            "Step: 16, loss: 0.34577836876104967\n",
            "Step: 16, loss: 0.34573684215413253\n",
            "Step: 16, loss: 0.34569739284971107\n",
            "Step: 16, loss: 0.34565653728006734\n",
            "Step: 16, loss: 0.34561350679505454\n",
            "Step: 16, loss: 0.3455697183452726\n",
            "Step: 16, loss: 0.34551836600136915\n",
            "Step: 16, loss: 0.34547433587508636\n",
            "Step: 16, loss: 0.3454226648422439\n",
            "Step: 16, loss: 0.34538170306859717\n",
            "Step: 16, loss: 0.3453463998560492\n",
            "Step: 16, loss: 0.3453203268565817\n",
            "STEP:  17\n",
            "Step: 17, loss: 0.34529685791840803\n",
            "Step: 17, loss: 0.34527313439525364\n",
            "Step: 17, loss: 0.3452528283898981\n",
            "Step: 17, loss: 0.34523366966001706\n",
            "Step: 17, loss: 0.34521614374801374\n",
            "Step: 17, loss: 0.34519956851534467\n",
            "Step: 17, loss: 0.34518334027895214\n",
            "Step: 17, loss: 0.34516864041805356\n",
            "Step: 17, loss: 0.34515496571133103\n",
            "Step: 17, loss: 0.34514200591266747\n",
            "Step: 17, loss: 0.3451276585146712\n",
            "Step: 17, loss: 0.34511189041057444\n",
            "Step: 17, loss: 0.34509464942770973\n",
            "Step: 17, loss: 0.3450766530299806\n",
            "Step: 17, loss: 0.34505676450864337\n",
            "Step: 17, loss: 0.34503534361034793\n",
            "Step: 17, loss: 0.34501092777319786\n",
            "Step: 17, loss: 0.3449832782827389\n",
            "Step: 17, loss: 0.34495354527220085\n",
            "Step: 17, loss: 0.34492037893551475\n",
            "STEP:  18\n",
            "Step: 18, loss: 0.3448885687508584\n",
            "Step: 18, loss: 0.34485113778996535\n",
            "Step: 18, loss: 0.3448226898877284\n",
            "Step: 18, loss: 0.3447871440975019\n",
            "Step: 18, loss: 0.34475127567961344\n",
            "Step: 18, loss: 0.3447062024687763\n",
            "Step: 18, loss: 0.34465851362368566\n",
            "Step: 18, loss: 0.3446017896210118\n",
            "Step: 18, loss: 0.34453773669325904\n",
            "Step: 18, loss: 0.34446511445082645\n",
            "Step: 18, loss: 0.3443794048863606\n",
            "Step: 18, loss: 0.3443207444896173\n",
            "Step: 18, loss: 0.3442470276978909\n",
            "Step: 18, loss: 0.3441633641454705\n",
            "Step: 18, loss: 0.3440961254612367\n",
            "Step: 18, loss: 0.3440274572998901\n",
            "Step: 18, loss: 0.34397668818069266\n",
            "Step: 18, loss: 0.34393826289383717\n",
            "Step: 18, loss: 0.34390263733235815\n",
            "Step: 18, loss: 0.34387065373912074\n",
            "STEP:  19\n",
            "Step: 19, loss: 0.343838915369227\n",
            "Step: 19, loss: 0.3438107690679076\n",
            "Step: 19, loss: 0.34378281253286913\n",
            "Step: 19, loss: 0.3437587581088015\n",
            "Step: 19, loss: 0.3437348192979645\n",
            "Step: 19, loss: 0.34371232388592965\n",
            "Step: 19, loss: 0.3436882374224115\n",
            "Step: 19, loss: 0.3436680703795204\n",
            "Step: 19, loss: 0.34364714588345985\n",
            "Step: 19, loss: 0.3436224123275485\n",
            "Step: 19, loss: 0.34358885471537964\n",
            "Step: 19, loss: 0.34356349142923887\n",
            "Step: 19, loss: 0.3435412894044729\n",
            "Step: 19, loss: 0.3435095079915023\n",
            "Step: 19, loss: 0.34346280588273204\n",
            "Step: 19, loss: 0.34340959620097816\n",
            "Step: 19, loss: 0.34335124145126666\n",
            "Step: 19, loss: 0.3432904734024858\n",
            "Step: 19, loss: 0.3432263847257349\n",
            "Step: 19, loss: 0.3431657800114379\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "batch_size = 16\n",
        "shuffle = True\n",
        "\n",
        "# Create a data loader\n",
        "train_features_tensor = torch.tensor(train[['Open','High','Low','Close','Volume','Adj Close']].values, dtype=torch.double)\n",
        "train_labels_tensor = torch.tensor(train['Cat_encoded'].values, dtype=torch.double)\n",
        "\n",
        "\n",
        "\n",
        "num_classes = 5\n",
        "one_hot_labels = F.one_hot(train_labels_tensor.view(-1).long(), num_classes)\n",
        "#train_labels_tensor = torch.unsqueeze(train_labels_tensor, dim=1)\n",
        "\n",
        "# Create a TensorDataset\n",
        "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)  # If you don't have labels, omit train_labels_tensor\n",
        "\n",
        "# Loading Training Data\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "input_length = 5\n",
        "hidden_length = 5\n",
        "lstm_cell = LSTM_cell(input_length, hidden_length)\n",
        "\n",
        "seq = Sequence()\n",
        "\n",
        "seq.double()\n",
        "\n",
        "#Initiatialising Cross Entropy Loss for Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "loss_history = []\n",
        "optimizer = optim.LBFGS(seq.parameters(), lr=0.1)\n",
        "# begin to train\n",
        "for i in range(20):\n",
        "  print('STEP: ', i)\n",
        "\n",
        "  def closure():\n",
        "      optimizer.zero_grad()\n",
        "      out = seq(train_features_tensor)\n",
        "      #print(f'out dimension {out.shape}')\n",
        "      loss = criterion(out, one_hot_labels)\n",
        "      loss_history.append(loss)\n",
        "      print(f\"loss: {loss.item()}\")\n",
        "      loss.backward()\n",
        "      return loss\n",
        "\n",
        "  optimizer.step(closure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSHWzMMQlCOn",
        "outputId": "b941285b-6016-42f0-c1e3-14c9a5346ec4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test loss: 0.6126722616860439\n"
          ]
        }
      ],
      "source": [
        "test_features_tensor = torch.tensor(test[['Open','High','Low','Close','Volume','Adj Close']].values, dtype=torch.double)\n",
        "test_labels_tensor = torch.tensor(test['Cat_encoded'].values, dtype=torch.double)\n",
        "num_classes = 5\n",
        "one_hot_labels_test = F.one_hot(test_labels_tensor.view(-1).long(), num_classes)\n",
        "with torch.no_grad():\n",
        "      future = 5\n",
        "      pred = seq(test_features_tensor, future=future)\n",
        "      class_probs = torch.softmax(pred, dim=1)\n",
        "      predicted_classes = torch.argmax(pred, dim=1)\n",
        "      loss = criterion(pred[:, :-future], one_hot_labels_test)\n",
        "      print('test loss:', loss.item())\n",
        "      y = pred.detach().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r6m4a_jiJqa",
        "outputId": "41895464-d665-4328-95d9-654c163103d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 43.0 %\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "# Convert PyTorch tensor to a numpy array\n",
        "#predicted_classes_np = predicted_classes.numpy()\n",
        "#predicted_classes = np.argmax(predicted_classes_np, axis=1)\n",
        "# Now `predicted_classes_np` contains the predicted class labels as integers\n",
        "\n",
        "# Assuming `test` is your pandas DataFrame containing ground truth labels\n",
        "# Access the ground truth labels from the DataFrame\n",
        "true_labels = test['Cat_encoded'].values\n",
        "predicted_classes = predicted_classes.numpy()\n",
        "predicted_classes = np.argmax(predicted_classes, axis=1)\n",
        "accuracy = (predicted_classes == true_labels).mean()\n",
        "print(\"Accuracy:\", accuracy * 100 ,'%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THwgZHYue_59"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "confusion_matrix = metrics.confusion_matrix(predicted_classes, true_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "H3yVt4U7feOr",
        "outputId": "8568b782-2a0e-4fda-e88d-10292d1a6f27"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2,3,4])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "73gkh8lcRvcU",
        "outputId": "8bb7f4e1-54e9-479e-8331-42a199a9a1f1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "loss_history = [x.item() for x in loss_history]\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('Batch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss History')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJPzSm_YR3yZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
